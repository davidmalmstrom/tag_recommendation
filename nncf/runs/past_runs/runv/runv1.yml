--nn_model: MLP
--is_tag: "1"
--eval_recall: "1"
--topk: "3"
--big_tag: "0"
--epochs: "300"
--layers: "[512,96]"
--reg_layers: "[0,0]"
--early_stopping: "45"
--test_dataset: "1"
--percentage: "0.1"
--dataset_name_prepend: "cold_0.1_"

# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566292013.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566292013.h5
# Load data done [1.5 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0239, Jaccard score = 0.0176
# Iteration 0 fit: [24.3 s]: Recall = 0.1565, Jaccard score = 0.1275, loss = 0.4343, eval: [7.9 s]
# Iteration 1 fit: [23.9 s]: Recall = 0.1849, Jaccard score = 0.1541, loss = 0.4019, eval: [8.1 s]
# Iteration 2 fit: [23.8 s]: Recall = 0.2006, Jaccard score = 0.1695, loss = 0.3729, eval: [8.1 s]
# Iteration 3 fit: [24.0 s]: Recall = 0.2032, Jaccard score = 0.1720, loss = 0.3469, eval: [7.9 s]
# Iteration 4 fit: [23.8 s]: Recall = 0.2051, Jaccard score = 0.1739, loss = 0.3233, eval: [7.9 s]
# Iteration 5 fit: [24.1 s]: Recall = 0.2011, Jaccard score = 0.1699, loss = 0.2988, eval: [8.0 s]
# Iteration 6 fit: [23.9 s]: Recall = 0.1969, Jaccard score = 0.1658, loss = 0.2750, eval: [8.0 s]
# Iteration 7 fit: [23.7 s]: Recall = 0.1972, Jaccard score = 0.1662, loss = 0.2536, eval: [8.0 s]
# Iteration 8 fit: [23.5 s]: Recall = 0.1957, Jaccard score = 0.1646, loss = 0.2327, eval: [8.0 s]
# Iteration 9 fit: [24.1 s]: Recall = 0.1947, Jaccard score = 0.1636, loss = 0.2137, eval: [8.1 s]
# Iteration 10 fit: [23.7 s]: Recall = 0.1919, Jaccard score = 0.1610, loss = 0.1958, eval: [8.1 s]
# Iteration 11 fit: [23.8 s]: Recall = 0.1874, Jaccard score = 0.1565, loss = 0.1788, eval: [8.1 s]
# Iteration 12 fit: [23.7 s]: Recall = 0.1932, Jaccard score = 0.1622, loss = 0.1643, eval: [8.1 s]
# Iteration 13 fit: [23.8 s]: Recall = 0.1913, Jaccard score = 0.1603, loss = 0.1507, eval: [8.1 s]
# Iteration 14 fit: [23.7 s]: Recall = 0.1890, Jaccard score = 0.1581, loss = 0.1377, eval: [8.1 s]
# Iteration 15 fit: [24.0 s]: Recall = 0.1883, Jaccard score = 0.1574, loss = 0.1266, eval: [8.0 s]
# Iteration 16 fit: [23.8 s]: Recall = 0.1900, Jaccard score = 0.1591, loss = 0.1161, eval: [8.0 s]
# Iteration 17 fit: [23.8 s]: Recall = 0.1898, Jaccard score = 0.1588, loss = 0.1069, eval: [7.9 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,96]_1566292699.h5
--weights_path: Pretrain/_MLP_8_[512,256,96]_1566292699.h5
# Load data done [1.6 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0339, Jaccard score = 0.0251
# Iteration 0 fit: [25.8 s]: Recall = 0.1866, Jaccard score = 0.1558, loss = 0.4217, eval: [8.3 s]
# Iteration 1 fit: [25.5 s]: Recall = 0.1974, Jaccard score = 0.1663, loss = 0.3780, eval: [8.3 s]
# Iteration 2 fit: [25.1 s]: Recall = 0.2024, Jaccard score = 0.1713, loss = 0.3487, eval: [8.3 s]
# Iteration 3 fit: [25.1 s]: Recall = 0.2012, Jaccard score = 0.1700, loss = 0.3205, eval: [8.2 s]
# Iteration 4 fit: [25.3 s]: Recall = 0.2008, Jaccard score = 0.1696, loss = 0.2919, eval: [8.1 s]
# Iteration 5 fit: [25.4 s]: Recall = 0.1973, Jaccard score = 0.1662, loss = 0.2641, eval: [8.3 s]
# Iteration 6 fit: [25.3 s]: Recall = 0.1942, Jaccard score = 0.1632, loss = 0.2384, eval: [8.2 s]
# Iteration 7 fit: [25.0 s]: Recall = 0.1896, Jaccard score = 0.1587, loss = 0.2150, eval: [8.3 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,96]_1566293133.h5
--weights_path: Pretrain/_MLP_8_[512,256,96]_1566293133.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0268, Jaccard score = 0.0198
# Iteration 0 fit: [29.4 s]: Recall = 0.1798, Jaccard score = 0.1493, loss = 0.4298, eval: [9.2 s]
# Iteration 1 fit: [28.4 s]: Recall = 0.1996, Jaccard score = 0.1685, loss = 0.3818, eval: [9.2 s]
# Iteration 2 fit: [28.5 s]: Recall = 0.1998, Jaccard score = 0.1687, loss = 0.3529, eval: [9.1 s]
# Iteration 3 fit: [28.5 s]: Recall = 0.1996, Jaccard score = 0.1684, loss = 0.3243, eval: [9.2 s]
# Iteration 4 fit: [28.5 s]: Recall = 0.1995, Jaccard score = 0.1684, loss = 0.2968, eval: [9.2 s]
# Iteration 5 fit: [28.5 s]: Recall = 0.1988, Jaccard score = 0.1677, loss = 0.2693, eval: [9.2 s]
# Iteration 6 fit: [30.0 s]: Recall = 0.1922, Jaccard score = 0.1613, loss = 0.2430, eval: [9.1 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,96]_1566293438.h5
--weights_path: Pretrain/_MLP_8_[512,256,96]_1566293438.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0295, Jaccard score = 0.0217
# Iteration 0 fit: [29.3 s]: Recall = 0.1862, Jaccard score = 0.1554, loss = 0.4224, eval: [9.2 s]
# Iteration 1 fit: [28.5 s]: Recall = 0.1992, Jaccard score = 0.1681, loss = 0.3797, eval: [9.3 s]
# Iteration 2 fit: [28.5 s]: Recall = 0.2036, Jaccard score = 0.1724, loss = 0.3517, eval: [9.2 s]
# Iteration 3 fit: [28.6 s]: Recall = 0.2003, Jaccard score = 0.1691, loss = 0.3234, eval: [9.3 s]
# Iteration 4 fit: [28.6 s]: Recall = 0.2024, Jaccard score = 0.1712, loss = 0.2954, eval: [9.2 s]
# Iteration 5 fit: [28.6 s]: Recall = 0.2005, Jaccard score = 0.1694, loss = 0.2686, eval: [9.3 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566293688.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566293688.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# 
# Performing k-fold 1
# Iteration 6 fit: [29.0 s]: Recall = 0.1997, Jaccard score = 0.1686, loss = 0.2439, eval: [9.2 s]
# Iteration 7 fit: [28.6 s]: Recall = 0.1922, Jaccard score = 0.1613, loss = 0.2202, eval: [9.2 s]
# Iteration 8 fit: [28.6 s]: Recall = 0.1916, Jaccard score = 0.1606, loss = 0.1990, eval: [9.1 s]
# Iteration 9 fit: [28.5 s]: Recall = 0.1912, Jaccard score = 0.1602, loss = 0.1802, eval: [9.3 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566293873.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566293873.h5
# Load data done [1.6 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0242, Jaccard score = 0.0178
# Iteration 0 fit: [31.9 s]: Recall = 0.1818, Jaccard score = 0.1512, loss = 0.4261, eval: [9.4 s]
# Iteration 1 fit: [30.6 s]: Recall = 0.2011, Jaccard score = 0.1699, loss = 0.3807, eval: [9.5 s]
# Iteration 2 fit: [30.6 s]: Recall = 0.2029, Jaccard score = 0.1718, loss = 0.3530, eval: [9.8 s]
# Iteration 3 fit: [33.1 s]: Recall = 0.2020, Jaccard score = 0.1708, loss = 0.3256, eval: [9.5 s]
# Iteration 4 fit: [31.0 s]: Recall = 0.1974, Jaccard score = 0.1663, loss = 0.2980, eval: [9.5 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566294125.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566294125.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# user_features (InputLayer)      (None, 1000)         0                                            
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1256)         0           flatten_1[0][0]                  
#                                                                  user_features[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 1512)         0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 256)          387328      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 256)          1024        layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# layer2 (Dense)                  (None, 128)          32896       leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 128)          512         layer2[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 128)          0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# layer3 (Dense)                  (None, 96)           12384       leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_3 (BatchNor (None, 96)           384         layer3[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 96)           0           batch_normalization_3[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_3[0][0]              
# ==================================================================================================
# Total params: 6,066,625
# Trainable params: 6,065,665
# Non-trainable params: 960
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0289, Jaccard score = 0.0213
# Iteration 0 fit: [32.3 s]: Recall = 0.1778, Jaccard score = 0.1474, loss = 0.4253, eval: [9.5 s]
# Iteration 1 fit: [31.1 s]: Recall = 0.1956, Jaccard score = 0.1645, loss = 0.3804, eval: [9.5 s]
# Iteration 2 fit: [31.1 s]: Recall = 0.1992, Jaccard score = 0.1681, loss = 0.3532, eval: [9.5 s]
# Iteration 3 fit: [31.1 s]: Recall = 0.2029, Jaccard score = 0.1718, loss = 0.3246, eval: [9.5 s]
# Iteration 4 fit: [31.1 s]: Recall = 0.2029, Jaccard score = 0.1717, loss = 0.2968, eval: [9.5 s]
# Iteration 5 fit: [31.1 s]: Recall = 0.1995, Jaccard score = 0.1684, loss = 0.2703, eval: [9.4 s]
# Iteration 6 fit: [31.0 s]: Recall = 0.1987, Jaccard score = 0.1675, loss = 0.2444, eval: [9.5 s]
# Iteration 7 fit: [31.1 s]: Recall = 0.1908, Jaccard score = 0.1599, loss = 0.2212, eval: [9.5 s]
# Iteration 8 fit: [31.0 s]: Recall = 0.1903, Jaccard score = 0.1594, loss = 0.1992, eval: [9.4 s]
# Iteration 9 fit: [31.1 s]: Recall = 0.1941, Jaccard score = 0.1631, loss = 0.1809, eval: [9.5 s]
# Iteration 10 fit: [31.0 s]: Recall = 0.1904, Jaccard score = 0.1595, loss = 0.1646, eval: [9.5 s]
# Iteration 11 fit: [31.1 s]: Recall = 0.1900, Jaccard score = 0.1591, loss = 0.1494, eval: [9.5 s]



# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566295669.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566295669.h5
# Load data done [1.6 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566295697.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566295697.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 512)          512512      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 512)          2048        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 768)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 1024)         0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 256)          262400      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 256)          1024        layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# layer2 (Dense)                  (None, 128)          32896       leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_3 (BatchNor (None, 128)          512         layer2[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           batch_normalization_3[0][0]      
# __________________________________________________________________________________________________
# layer3 (Dense)                  (None, 96)           12384       leaky_re_lu_3[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_4 (BatchNor (None, 96)           384         layer3[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_4 (LeakyReLU)       (None, 96)           0           batch_normalization_4[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_4[0][0]              
# ==================================================================================================
# Total params: 6,456,257
# Trainable params: 6,454,273
# Non-trainable params: 1,984
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0316, Jaccard score = 0.0233
# Iteration 0 fit: [34.2 s]: Recall = 0.1942, Jaccard score = 0.1631, loss = 0.4213, eval: [9.9 s]
# Iteration 1 fit: [32.5 s]: Recall = 0.2070, Jaccard score = 0.1758, loss = 0.3721, eval: [10.0 s]
# Iteration 2 fit: [32.5 s]: Recall = 0.2102, Jaccard score = 0.1791, loss = 0.3448, eval: [9.8 s]
# Iteration 3 fit: [33.0 s]: Recall = 0.2152, Jaccard score = 0.1840, loss = 0.3187, eval: [9.9 s]
# Iteration 4 fit: [32.5 s]: Recall = 0.2120, Jaccard score = 0.1809, loss = 0.2942, eval: [10.0 s]
# Iteration 5 fit: [32.5 s]: Recall = 0.2117, Jaccard score = 0.1805, loss = 0.2709, eval: [9.8 s]
# Iteration 6 fit: [32.5 s]: Recall = 0.2115, Jaccard score = 0.1804, loss = 0.2491, eval: [9.8 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566296564.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566296564.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 512)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 768)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 256)          196864      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 256)          1024        layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# layer2 (Dense)                  (None, 128)          32896       leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_3 (BatchNor (None, 128)          512         layer2[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           batch_normalization_3[0][0]      
# __________________________________________________________________________________________________
# layer3 (Dense)                  (None, 96)           12384       leaky_re_lu_3[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_4 (BatchNor (None, 96)           384         layer3[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_4 (LeakyReLU)       (None, 96)           0           batch_normalization_4[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_4[0][0]              
# ==================================================================================================
# Total params: 6,133,441
# Trainable params: 6,131,969
# Non-trainable params: 1,472
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0288, Jaccard score = 0.0212
# Iteration 0 fit: [35.6 s]: Recall = 0.1975, Jaccard score = 0.1664, loss = 0.4214, eval: [10.2 s]
# Iteration 1 fit: [32.3 s]: Recall = 0.2055, Jaccard score = 0.1743, loss = 0.3708, eval: [9.9 s]
# Iteration 2 fit: [32.3 s]: Recall = 0.2102, Jaccard score = 0.1791, loss = 0.3430, eval: [10.0 s]
# Iteration 3 fit: [32.4 s]: Recall = 0.2131, Jaccard score = 0.1820, loss = 0.3172, eval: [10.0 s]
# Iteration 4 fit: [32.2 s]: Recall = 0.2090, Jaccard score = 0.1778, loss = 0.2917, eval: [10.1 s]
# Iteration 5 fit: [32.3 s]: Recall = 0.2086, Jaccard score = 0.1774, loss = 0.2678, eval: [10.0 s]
# Iteration 6 fit: [32.2 s]: Recall = 0.2070, Jaccard score = 0.1758, loss = 0.2456, eval: [10.0 s]
# Iteration 7 fit: [32.3 s]: Recall = 0.2063, Jaccard score = 0.1752, loss = 0.2246, eval: [10.0 s]
# Iteration 8 fit: [32.3 s]: Recall = 0.2003, Jaccard score = 0.1692, loss = 0.2045, eval: [10.1 s]
# Iteration 9 fit: [32.2 s]: Recall = 0.1999, Jaccard score = 0.1687, loss = 0.1865, eval: [10.0 s]
# Iteration 10 fit: [32.2 s]: Recall = 0.2002, Jaccard score = 0.1691, loss = 0.1713, eval: [10.2 s]
# Iteration 11 fit: [31.8 s]: Recall = 0.2038, Jaccard score = 0.1727, loss = 0.1561, eval: [10.0 s]
# Iteration 12 fit: [32.2 s]: Recall = 0.1971, Jaccard score = 0.1660, loss = 0.1443, eval: [10.0 s]
# Iteration 13 fit: [32.3 s]: Recall = 0.1962, Jaccard score = 0.1651, loss = 0.1331, eval: [10.0 s]
# Iteration 14 fit: [32.1 s]: Recall = 0.1983, Jaccard score = 0.1672, loss = 0.1233, eval: [10.0 s]
# Iteration 15 fit: [32.2 s]: Recall = 0.1985, Jaccard score = 0.1674, loss = 0.1155, eval: [10.0 s]
# Iteration 16 fit: [32.4 s]: Recall = 0.2005, Jaccard score = 0.1694, loss = 0.1078, eval: [10.1 s]
# Iteration 17 fit: [31.8 s]: Recall = 0.1960, Jaccard score = 0.1650, loss = 0.1021, eval: [10.0 s]
# Iteration 18 fit: [32.0 s]: Recall = 0.1999, Jaccard score = 0.1687, loss = 0.0956, eval: [9.9 s]
# Iteration 19 fit: [32.2 s]: Recall = 0.1955, Jaccard score = 0.1645, loss = 0.0912, eval: [10.0 s]
# Iteration 20 fit: [32.2 s]: Recall = 0.1999, Jaccard score = 0.1688, loss = 0.0855, eval: [10.1 s]
# Iteration 21 fit: [32.2 s]: Recall = 0.1982, Jaccard score = 0.1671, loss = 0.0818, eval: [10.2 s]
# Iteration 22 fit: [32.2 s]: Recall = 0.1986, Jaccard score = 0.1675, loss = 0.0777, eval: [10.0 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566304182.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566304182.h5
# Load data done [1.9 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 512)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 768)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 256)          196864      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 256)          1024        layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# layer2 (Dense)                  (None, 128)          32896       leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_3 (BatchNor (None, 128)          512         layer2[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           batch_normalization_3[0][0]      
# __________________________________________________________________________________________________
# layer3 (Dense)                  (None, 96)           12384       leaky_re_lu_3[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_4 (BatchNor (None, 96)           384         layer3[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_4 (LeakyReLU)       (None, 96)           0           batch_normalization_4[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_4[0][0]              
# ==================================================================================================
# Total params: 6,133,441
# Trainable params: 6,131,969
# Non-trainable params: 1,472
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0277, Jaccard score = 0.0204
# Iteration 0 fit: [65.6 s]: Recall = 0.2060, Jaccard score = 0.1748, loss = 0.3684, eval: [10.2 s]
# Iteration 1 fit: [61.9 s]: Recall = 0.2090, Jaccard score = 0.1778, loss = 0.3090, eval: [10.1 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566304379.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566304379.h5
# Load data done [4.1 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 512)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 768)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 256)          196864      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 256)          1024        layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# layer2 (Dense)                  (None, 128)          32896       leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_3 (BatchNor (None, 128)          512         layer2[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           batch_normalization_3[0][0]      
# __________________________________________________________________________________________________
# layer3 (Dense)                  (None, 96)           12384       leaky_re_lu_3[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_4 (BatchNor (None, 96)           384         layer3[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_4 (LeakyReLU)       (None, 96)           0           batch_normalization_4[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_4[0][0]              
# ==================================================================================================
# Total params: 6,133,441
# Trainable params: 6,131,969
# Non-trainable params: 1,472
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0257, Jaccard score = 0.0189


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566304617.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566304617.h5
# Load data done [1.5 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 512)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 768)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 256)          196864      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 256)          1024        layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# layer2 (Dense)                  (None, 128)          32896       leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_3 (BatchNor (None, 128)          512         layer2[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           batch_normalization_3[0][0]      
# __________________________________________________________________________________________________
# layer3 (Dense)                  (None, 96)           12384       leaky_re_lu_3[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_4 (BatchNor (None, 96)           384         layer3[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_4 (LeakyReLU)       (None, 96)           0           batch_normalization_4[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_4[0][0]              
# ==================================================================================================
# Total params: 6,133,441
# Trainable params: 6,131,969
# Non-trainable params: 1,472
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0262, Jaccard score = 0.0193
# Iteration 0 fit: [65.9 s]: Recall = 0.2041, Jaccard score = 0.1729, loss = 0.3688, val_loss 0.4956, eval: [10.2 s]
# Iteration 1 fit: [62.3 s]: Recall = 0.2079, Jaccard score = 0.1768, loss = 0.3096, val_loss 0.5742, eval: [10.2 s]
# Iteration 2 fit: [62.4 s]: Recall = 0.2020, Jaccard score = 0.1709, loss = 0.2522, val_loss 0.6896, eval: [10.2 s]
# Iteration 3 fit: [62.4 s]: Recall = 0.2006, Jaccard score = 0.1694, loss = 0.2028, val_loss 0.8234, eval: [10.2 s]
# Iteration 4 fit: [62.5 s]: Recall = 0.1933, Jaccard score = 0.1623, loss = 0.1624, val_loss 1.0220, eval: [10.2 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566306749.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566306749.h5
# Load data done [4.1 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 512)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 768)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 256)          196864      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 256)          1024        layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# layer2 (Dense)                  (None, 128)          32896       leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_3 (BatchNor (None, 128)          512         layer2[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           batch_normalization_3[0][0]      
# __________________________________________________________________________________________________
# layer3 (Dense)                  (None, 96)           12384       leaky_re_lu_3[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_4 (BatchNor (None, 96)           384         layer3[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_4 (LeakyReLU)       (None, 96)           0           batch_normalization_4[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_4[0][0]              
# ==================================================================================================
# Total params: 6,133,441
# Trainable params: 6,131,969
# Non-trainable params: 1,472
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0245, Jaccard score = 0.0180


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,256,128,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,256,128,96]_1566307864.h5
--weights_path: Pretrain/_MLP_8_[512,256,128,96]_1566307864.h5
# Load data done [4.1 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 512)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 768)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 256)          196864      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 256)          1024        layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# layer2 (Dense)                  (None, 128)          32896       leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_3 (BatchNor (None, 128)          512         layer2[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           batch_normalization_3[0][0]      
# __________________________________________________________________________________________________
# layer3 (Dense)                  (None, 96)           12384       leaky_re_lu_3[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_4 (BatchNor (None, 96)           384         layer3[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_4 (LeakyReLU)       (None, 96)           0           batch_normalization_4[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_4[0][0]              
# ==================================================================================================
# Total params: 6,133,441
# Trainable params: 6,131,969
# Non-trainable params: 1,472
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0259, Jaccard score = 0.0191


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566381203.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566381203.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 512)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 768)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 96)           73824       concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 96)           384         layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 5,963,585
# Trainable params: 5,962,881
# Non-trainable params: 704
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0266, Jaccard score = 0.0196
# Iteration 0 fit: [27.4 s]: Recall = 0.1847, Jaccard score = 0.1539, loss = 0.4252, eval: [9.0 s]
# Iteration 1 fit: [26.4 s]: Recall = 0.2019, Jaccard score = 0.1707, loss = 0.3816, eval: [9.0 s]
# Iteration 2 fit: [26.3 s]: Recall = 0.2108, Jaccard score = 0.1796, loss = 0.3568, eval: [8.9 s]
# Iteration 3 fit: [26.4 s]: Recall = 0.2120, Jaccard score = 0.1808, loss = 0.3355, eval: [8.9 s]
# Iteration 4 fit: [26.4 s]: Recall = 0.2130, Jaccard score = 0.1818, loss = 0.3155, eval: [8.9 s]
# Iteration 5 fit: [26.4 s]: Recall = 0.2118, Jaccard score = 0.1806, loss = 0.2960, eval: [8.9 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566381468.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566381468.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 128)          128128      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 128)          512         feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 384)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 640)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 96)           61536       concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 96)           384         layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 5,822,657
# Trainable params: 5,822,209
# Non-trainable params: 448
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0265, Jaccard score = 0.0195
# Iteration 0 fit: [26.9 s]: Recall = 0.1819, Jaccard score = 0.1512, loss = 0.4277, eval: [9.2 s]
# Iteration 1 fit: [26.0 s]: Recall = 0.2005, Jaccard score = 0.1694, loss = 0.3840, eval: [9.1 s]
# Iteration 2 fit: [26.0 s]: Recall = 0.2066, Jaccard score = 0.1754, loss = 0.3601, eval: [9.2 s]
# Iteration 3 fit: [26.0 s]: Recall = 0.2106, Jaccard score = 0.1794, loss = 0.3401, eval: [9.2 s]
# Iteration 4 fit: [26.0 s]: Recall = 0.2094, Jaccard score = 0.1782, loss = 0.3197, eval: [9.2 s]
# Iteration 5 fit: [26.0 s]: Recall = 0.2092, Jaccard score = 0.1780, loss = 0.3012, eval: [9.3 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566381701.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566381701.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566381842.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566381842.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 342)          342342      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 342)          1368        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 342)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 598)          0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 854)          0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 96)           82080       concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 96)           384         layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 6,058,271
# Trainable params: 6,057,395
# Non-trainable params: 876
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0340, Jaccard score = 0.0251
# Iteration 0 fit: [27.8 s]: Recall = 0.1823, Jaccard score = 0.1516, loss = 0.4256, eval: [8.9 s]
# Iteration 1 fit: [26.8 s]: Recall = 0.2029, Jaccard score = 0.1717, loss = 0.3835, eval: [8.9 s]
# Iteration 2 fit: [26.7 s]: Recall = 0.2120, Jaccard score = 0.1809, loss = 0.3584, eval: [9.0 s]
# Iteration 3 fit: [26.7 s]: Recall = 0.2113, Jaccard score = 0.1801, loss = 0.3363, eval: [9.0 s]
# Iteration 4 fit: [26.7 s]: Recall = 0.2127, Jaccard score = 0.1815, loss = 0.3149, eval: [9.0 s]
# Iteration 5 fit: [26.8 s]: Recall = 0.2122, Jaccard score = 0.1811, loss = 0.2961, eval: [8.9 s]
# Iteration 6 fit: [26.7 s]: Recall = 0.2113, Jaccard score = 0.1802, loss = 0.2770, eval: [8.9 s]
# Iteration 7 fit: [26.8 s]: Recall = 0.2133, Jaccard score = 0.1822, loss = 0.2593, eval: [9.0 s]
# Iteration 8 fit: [26.7 s]: Recall = 0.2117, Jaccard score = 0.1805, loss = 0.2429, eval: [9.0 s]
# Iteration 9 fit: [26.8 s]: Recall = 0.2091, Jaccard score = 0.1779, loss = 0.2272, eval: [9.0 s]
# Iteration 10 fit: [26.7 s]: Recall = 0.2073, Jaccard score = 0.1761, loss = 0.2128, eval: [9.0 s]
# Iteration 11 fit: [26.8 s]: Recall = 0.2074, Jaccard score = 0.1762, loss = 0.1994, eval: [8.9 s]
# Iteration 12 fit: [26.7 s]: Recall = 0.2078, Jaccard score = 0.1766, loss = 0.1853, eval: [8.9 s]
# Iteration 13 fit: [26.7 s]: Recall = 0.2086, Jaccard score = 0.1774, loss = 0.1739, eval: [9.0 s]
# Iteration 14 fit: [26.7 s]: Recall = 0.2042, Jaccard score = 0.1731, loss = 0.1621, eval: [9.0 s]
# Iteration 15 fit: [26.8 s]: Recall = 0.2065, Jaccard score = 0.1753, loss = 0.1513, eval: [9.0 s]
# Iteration 16 fit: [26.7 s]: Recall = 0.2033, Jaccard score = 0.1721, loss = 0.1407, eval: [8.9 s]
# Iteration 17 fit: [26.8 s]: Recall = 0.2045, Jaccard score = 0.1733, loss = 0.1321, eval: [9.0 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566382544.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566382544.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 768)          768768      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 768)          3072        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 768)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1024)         0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 1280)         0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 96)           122976      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 96)           384         layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 6,527,297
# Trainable params: 6,525,569
# Non-trainable params: 1,728
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0224, Jaccard score = 0.0165
# Iteration 0 fit: [29.2 s]: Recall = 0.1853, Jaccard score = 0.1546, loss = 0.4246, eval: [9.0 s]
# Iteration 1 fit: [28.3 s]: Recall = 0.1978, Jaccard score = 0.1667, loss = 0.3808, eval: [9.1 s]
# Iteration 2 fit: [28.3 s]: Recall = 0.2087, Jaccard score = 0.1775, loss = 0.3588, eval: [9.1 s]
# Iteration 3 fit: [28.3 s]: Recall = 0.2111, Jaccard score = 0.1800, loss = 0.3398, eval: [9.1 s]
# Iteration 4 fit: [28.3 s]: Recall = 0.2120, Jaccard score = 0.1808, loss = 0.3209, eval: [8.9 s]
# Iteration 5 fit: [28.4 s]: Recall = 0.2140, Jaccard score = 0.1828, loss = 0.3036, eval: [9.1 s]
# Iteration 6 fit: [29.7 s]: Recall = 0.2143, Jaccard score = 0.1832, loss = 0.2873, eval: [9.1 s]
# Iteration 7 fit: [28.3 s]: Recall = 0.2163, Jaccard score = 0.1852, loss = 0.2721, eval: [9.0 s]
# Iteration 8 fit: [28.3 s]: Recall = 0.2160, Jaccard score = 0.1848, loss = 0.2580, eval: [9.0 s]
# Iteration 9 fit: [28.3 s]: Recall = 0.2124, Jaccard score = 0.1813, loss = 0.2449, eval: [9.1 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566382947.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566382947.h5
# Load data done [1.6 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 1024)         1025024     user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 1024)         4096        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 1024)         0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1280)         0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 1536)         0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 96)           147552      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 96)           384         layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 6,809,153
# Trainable params: 6,806,913
# Non-trainable params: 2,240
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0254, Jaccard score = 0.0187
# Iteration 0 fit: [30.4 s]: Recall = 0.1844, Jaccard score = 0.1536, loss = 0.4264, eval: [9.1 s]
# Iteration 1 fit: [29.3 s]: Recall = 0.2005, Jaccard score = 0.1694, loss = 0.3830, eval: [9.2 s]
# Iteration 2 fit: [29.4 s]: Recall = 0.2072, Jaccard score = 0.1760, loss = 0.3594, eval: [9.1 s]
# Iteration 3 fit: [29.4 s]: Recall = 0.2128, Jaccard score = 0.1816, loss = 0.3396, eval: [9.1 s]
# Iteration 4 fit: [29.3 s]: Recall = 0.2079, Jaccard score = 0.1768, loss = 0.3206, eval: [9.1 s]
# Iteration 5 fit: [29.6 s]: Recall = 0.2133, Jaccard score = 0.1822, loss = 0.3036, eval: [9.2 s]
# Iteration 6 fit: [30.6 s]: Recall = 0.2094, Jaccard score = 0.1782, loss = 0.2867, eval: [9.2 s]


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[512,96]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_MLP_8_[512,96]_1566383249.h5
--weights_path: Pretrain/_MLP_8_[512,96]_1566383249.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 768)          768768      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 256)       5120000     user_input[0][0]                 
# __________________________________________________________________________________________________
# batch_normalization_1 (BatchNor (None, 768)          3072        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 256)          0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 768)          0           batch_normalization_1[0][0]      
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 256)       512000      item_input[0][0]                 
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1024)         0           flatten_1[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 256)          0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 1280)         0           concatenate_1[0][0]              
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# layer1 (Dense)                  (None, 96)           122976      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# batch_normalization_2 (BatchNor (None, 96)           384         layer1[0][0]                     
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           batch_normalization_2[0][0]      
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 6,527,297
# Trainable params: 6,525,569
# Non-trainable params: 1,728
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0335, Jaccard score = 0.0248
# Iteration 0 fit: [29.3 s]: Recall = 0.1839, Jaccard score = 0.1532, loss = 0.4246, eval: [9.1 s]
# Iteration 1 fit: [28.7 s]: Recall = 0.1984, Jaccard score = 0.1673, loss = 0.3818, eval: [9.1 s]
# Iteration 2 fit: [28.6 s]: Recall = 0.2083, Jaccard score = 0.1771, loss = 0.3607, eval: [9.1 s]
# Iteration 3 fit: [28.7 s]: Recall = 0.2120, Jaccard score = 0.1809, loss = 0.3406, eval: [9.0 s]
# Iteration 4 fit: [28.7 s]: Recall = 0.2086, Jaccard score = 0.1774, loss = 0.3211, eval: [9.0 s]
# Iteration 5 fit: [28.7 s]: Recall = 0.2115, Jaccard score = 0.1804, loss = 0.3027, eval: [9.1 s]
# Iteration 6 fit: [28.6 s]: Recall = 0.2084, Jaccard score = 0.1772, loss = 0.2855, eval: [9.1 s]
# Iteration 7 fit: [28.6 s]: Recall = 0.2113, Jaccard score = 0.1801, loss = 0.2697, eval: [9.1 s]
# Iteration 8 fit: [28.6 s]: Recall = 0.2095, Jaccard score = 0.1783, loss = 0.2540, eval: [9.1 s]
# Iteration 9 fit: [28.6 s]: Recall = 0.2070, Jaccard score = 0.1758, loss = 0.2399, eval: [9.1 s]
# Iteration 10 fit: [28.6 s]: Recall = 0.2073, Jaccard score = 0.1761, loss = 0.2264, eval: [9.1 s]
# Iteration 11 fit: [28.7 s]: Recall = 0.2056, Jaccard score = 0.1745, loss = 0.2136, eval: [9.1 s]
# Iteration 12 fit: [28.6 s]: Recall = 0.2061, Jaccard score = 0.1750, loss = 0.2007, eval: [9.1 s]
# Iteration 13 fit: [28.7 s]: Recall = 0.2072, Jaccard score = 0.1760, loss = 0.1894, eval: [9.1 s]
# Iteration 14 fit: [28.6 s]: Recall = 0.2037, Jaccard score = 0.1725, loss = 0.1765, eval: [9.1 s]
# Iteration 15 fit: [28.7 s]: Recall = 0.2032, Jaccard score = 0.1720, loss = 0.1668, eval: [9.0 s]
# Iteration 16 fit: [28.6 s]: Recall = 0.2025, Jaccard score = 0.1713, loss = 0.1566, eval: [9.1 s]
# Iteration 17 fit: [28.7 s]: Recall = 0.2024, Jaccard score = 0.1712, loss = 0.1474, eval: [9.1 s]
# Iteration 18 fit: [28.4 s]: Recall = 0.2017, Jaccard score = 0.1705, loss = 0.1382, eval: [9.1 s]
# Iteration 19 fit: [28.7 s]: Recall = 0.2040, Jaccard score = 0.1729, loss = 0.1299, eval: [9.0 s]
# Iteration 20 fit: [28.6 s]: Recall = 0.2037, Jaccard score = 0.1725, loss = 0.1208, eval: [9.1 s]
# Iteration 21 fit: [28.7 s]: Recall = 0.2037, Jaccard score = 0.1725, loss = 0.1142, eval: [9.1 s]
# Iteration 22 fit: [28.6 s]: Recall = 0.2026, Jaccard score = 0.1715, loss = 0.1084, eval: [9.1 s]
# Iteration 23 fit: [28.7 s]: Recall = 0.2044, Jaccard score = 0.1732, loss = 0.1014, eval: [9.1 s]
# Iteration 24 fit: [28.6 s]: Recall = 0.2023, Jaccard score = 0.1711, loss = 0.0956, eval: [9.1 s]
# Iteration 25 fit: [28.7 s]: Recall = 0.2049, Jaccard score = 0.1737, loss = 0.0911, eval: [9.1 s]
# Iteration 26 fit: [28.7 s]: Recall = 0.2024, Jaccard score = 0.1712, loss = 0.0862, eval: [9.1 s]
# Iteration 27 fit: [28.7 s]: Recall = 0.1996, Jaccard score = 0.1685, loss = 0.0816, eval: [9.1 s]
# Iteration 28 fit: [28.6 s]: Recall = 0.2033, Jaccard score = 0.1722, loss = 0.0777, eval: [9.1 s]
# Iteration 29 fit: [28.7 s]: Recall = 0.2037, Jaccard score = 0.1725, loss = 0.0743, eval: [9.1 s]
