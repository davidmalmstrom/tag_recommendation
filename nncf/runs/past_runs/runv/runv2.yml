--nn_model: GMF
--is_tag: "1"
--eval_recall: "1"
--topk: "3"
--big_tag: "0"
--epochs: "300"
--num_factors: "94"
--early_stopping: "45"
--test_dataset: "1"
--percentage: "0.0"
--dataset_name_prepend: "cold_0.0_"

# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566305071.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566305071.h5
# Load data done [2.0 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0310, Jaccard score = 0.0187
# Iteration 0 fit: [22.1 s]: Recall = 0.0299, Jaccard score = 0.0181, loss = 0.5007, val_loss 0.5012, eval: [6.5 s]
# Iteration 1 fit: [19.9 s]: Recall = 0.0328, Jaccard score = 0.0198, loss = 0.4922, val_loss 0.5104, eval: [6.4 s]
# Iteration 2 fit: [19.9 s]: Recall = 0.0408, Jaccard score = 0.0248, loss = 0.4587, val_loss 0.5464, eval: [6.4 s]
# Iteration 3 fit: [19.9 s]: Recall = 0.0600, Jaccard score = 0.0369, loss = 0.3961, val_loss 0.6143, eval: [6.4 s]
# Iteration 4 fit: [19.9 s]: Recall = 0.0819, Jaccard score = 0.0511, loss = 0.3168, val_loss 0.7142, eval: [6.4 s]
# Iteration 5 fit: [19.9 s]: Recall = 0.1019, Jaccard score = 0.0643, loss = 0.2410, val_loss 0.8386, eval: [6.4 s]
# Iteration 6 fit: [19.9 s]: Recall = 0.1183, Jaccard score = 0.0755, loss = 0.1790, val_loss 0.9974, eval: [6.4 s]
# Iteration 7 fit: [19.9 s]: Recall = 0.1294, Jaccard score = 0.0832, loss = 0.1303, val_loss 1.1805, eval: [6.4 s]
# Iteration 8 fit: [19.9 s]: Recall = 0.1323, Jaccard score = 0.0852, loss = 0.0929, val_loss 1.3874, eval: [6.5 s]
# Iteration 9 fit: [20.0 s]: Recall = 0.1444, Jaccard score = 0.0937, loss = 0.0655, val_loss 1.6091, eval: [6.4 s]
# Iteration 10 fit: [19.8 s]: Recall = 0.1463, Jaccard score = 0.0951, loss = 0.0450, val_loss 1.8323, eval: [6.4 s]
# Iteration 11 fit: [19.9 s]: Recall = 0.1554, Jaccard score = 0.1015, loss = 0.0311, val_loss 2.0335, eval: [6.4 s]
# Iteration 12 fit: [19.9 s]: Recall = 0.1639, Jaccard score = 0.1077, loss = 0.0228, val_loss 2.2108, eval: [6.4 s]
# Iteration 13 fit: [19.9 s]: Recall = 0.1724, Jaccard score = 0.1139, loss = 0.0170, val_loss 2.3591, eval: [6.4 s]
# Iteration 14 fit: [19.9 s]: Recall = 0.1813, Jaccard score = 0.1205, loss = 0.0127, val_loss 2.4864, eval: [6.4 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566305667.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566305667.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0299, Jaccard score = 0.0181
# Iteration 0 fit: [12.8 s]: Recall = 0.0295, Jaccard score = 0.0178, loss = 0.5379, val_loss 0.5009, eval: [6.4 s]
# Iteration 1 fit: [10.5 s]: Recall = 0.0291, Jaccard score = 0.0176, loss = 0.5009, val_loss 0.5011, eval: [6.4 s]
# Iteration 2 fit: [10.5 s]: Recall = 0.0268, Jaccard score = 0.0162, loss = 0.4998, val_loss 0.5024, eval: [6.4 s]
# Iteration 3 fit: [10.5 s]: Recall = 0.0295, Jaccard score = 0.0178, loss = 0.4959, val_loss 0.5075, eval: [6.4 s]
# Iteration 4 fit: [10.5 s]: Recall = 0.0334, Jaccard score = 0.0202, loss = 0.4866, val_loss 0.5175, eval: [6.4 s]
# Iteration 5 fit: [10.5 s]: Recall = 0.0383, Jaccard score = 0.0232, loss = 0.4710, val_loss 0.5338, eval: [6.3 s]
# Iteration 6 fit: [10.5 s]: Recall = 0.0484, Jaccard score = 0.0296, loss = 0.4482, val_loss 0.5552, eval: [6.3 s]
# Iteration 7 fit: [10.5 s]: Recall = 0.0567, Jaccard score = 0.0348, loss = 0.4176, val_loss 0.5893, eval: [6.3 s]
# Iteration 8 fit: [10.5 s]: Recall = 0.0682, Jaccard score = 0.0422, loss = 0.3827, val_loss 0.6246, eval: [6.4 s]
# Iteration 9 fit: [10.5 s]: Recall = 0.0806, Jaccard score = 0.0503, loss = 0.3452, val_loss 0.6679, eval: [6.4 s]
# Iteration 10 fit: [10.5 s]: Recall = 0.0858, Jaccard score = 0.0536, loss = 0.3077, val_loss 0.7185, eval: [6.4 s]
# Iteration 11 fit: [10.5 s]: Recall = 0.1020, Jaccard score = 0.0644, loss = 0.2726, val_loss 0.7743, eval: [6.4 s]
# Iteration 12 fit: [10.5 s]: Recall = 0.1111, Jaccard score = 0.0706, loss = 0.2400, val_loss 0.8349, eval: [6.4 s]
# Iteration 13 fit: [10.5 s]: Recall = 0.1138, Jaccard score = 0.0724, loss = 0.2115, val_loss 0.9017, eval: [6.4 s]
# Iteration 14 fit: [10.5 s]: Recall = 0.1198, Jaccard score = 0.0765, loss = 0.1846, val_loss 0.9819, eval: [6.4 s]
# Iteration 15 fit: [10.5 s]: Recall = 0.1233, Jaccard score = 0.0789, loss = 0.1610, val_loss 1.0583, eval: [6.4 s]
# Iteration 16 fit: [10.5 s]: Recall = 0.1293, Jaccard score = 0.0831, loss = 0.1403, val_loss 1.1530, eval: [6.4 s]
# Iteration 17 fit: [10.5 s]: Recall = 0.1276, Jaccard score = 0.0819, loss = 0.1212, val_loss 1.2545, eval: [6.3 s]
# Iteration 18 fit: [10.5 s]: Recall = 0.1350, Jaccard score = 0.0871, loss = 0.1050, val_loss 1.3553, eval: [6.4 s]
# Iteration 19 fit: [10.5 s]: Recall = 0.1358, Jaccard score = 0.0877, loss = 0.0909, val_loss 1.4714, eval: [6.4 s]
# Iteration 20 fit: [10.5 s]: Recall = 0.1426, Jaccard score = 0.0924, loss = 0.0781, val_loss 1.5850, eval: [6.4 s]
# Iteration 21 fit: [10.5 s]: Recall = 0.1439, Jaccard score = 0.0934, loss = 0.0675, val_loss 1.6991, eval: [6.4 s]
# Iteration 22 fit: [10.5 s]: Recall = 0.1507, Jaccard score = 0.0982, loss = 0.0585, val_loss 1.8123, eval: [6.4 s]
# Iteration 23 fit: [10.5 s]: Recall = 0.1558, Jaccard score = 0.1019, loss = 0.0509, val_loss 1.9213, eval: [6.4 s]
# Iteration 24 fit: [10.5 s]: Recall = 0.1607, Jaccard score = 0.1054, loss = 0.0438, val_loss 2.0264, eval: [6.3 s]
# Iteration 25 fit: [10.5 s]: Recall = 0.1663, Jaccard score = 0.1095, loss = 0.0384, val_loss 2.1265, eval: [6.4 s]
# Iteration 26 fit: [10.5 s]: Recall = 0.1700, Jaccard score = 0.1122, loss = 0.0335, val_loss 2.2105, eval: [6.4 s]
# Iteration 27 fit: [10.5 s]: Recall = 0.1724, Jaccard score = 0.1139, loss = 0.0298, val_loss 2.2959, eval: [6.4 s]
# Iteration 28 fit: [10.5 s]: Recall = 0.1780, Jaccard score = 0.1181, loss = 0.0267, val_loss 2.3687, eval: [6.4 s]
# Iteration 29 fit: [10.5 s]: Recall = 0.1827, Jaccard score = 0.1216, loss = 0.0237, val_loss 2.4340, eval: [6.4 s]
# Iteration 30 fit: [10.5 s]: Recall = 0.1819, Jaccard score = 0.1210, loss = 0.0212, val_loss 2.5044, eval: [6.4 s]
# Iteration 31 fit: [10.6 s]: Recall = 0.1919, Jaccard score = 0.1285, loss = 0.0190, val_loss 2.5607, eval: [6.4 s]
# Iteration 32 fit: [10.5 s]: Recall = 0.1925, Jaccard score = 0.1289, loss = 0.0171, val_loss 2.6073, eval: [6.4 s]
# Iteration 33 fit: [10.5 s]: Recall = 0.1977, Jaccard score = 0.1329, loss = 0.0157, val_loss 2.6574, eval: [6.4 s]
# Iteration 34 fit: [10.5 s]: Recall = 0.2031, Jaccard score = 0.1370, loss = 0.0141, val_loss 2.7011, eval: [6.4 s]
# Iteration 35 fit: [10.5 s]: Recall = 0.2084, Jaccard score = 0.1411, loss = 0.0129, val_loss 2.7355, eval: [6.4 s]
# Iteration 36 fit: [10.5 s]: Recall = 0.2154, Jaccard score = 0.1466, loss = 0.0118, val_loss 2.7731, eval: [6.4 s]
# Iteration 37 fit: [10.5 s]: Recall = 0.2149, Jaccard score = 0.1461, loss = 0.0110, val_loss 2.7975, eval: [6.3 s]
# Iteration 38 fit: [10.5 s]: Recall = 0.2180, Jaccard score = 0.1485, loss = 0.0100, val_loss 2.8332, eval: [6.3 s]
# Iteration 39 fit: [10.5 s]: Recall = 0.2205, Jaccard score = 0.1505, loss = 0.0095, val_loss 2.8560, eval: [6.4 s]
# Iteration 40 fit: [10.5 s]: Recall = 0.2296, Jaccard score = 0.1577, loss = 0.0090, val_loss 2.8801, eval: [6.3 s]
# Iteration 41 fit: [10.5 s]: Recall = 0.2332, Jaccard score = 0.1605, loss = 0.0084, val_loss 2.9031, eval: [6.4 s]
# Iteration 42 fit: [10.5 s]: Recall = 0.2373, Jaccard score = 0.1638, loss = 0.0078, val_loss 2.9181, eval: [6.4 s]
# Iteration 43 fit: [10.5 s]: Recall = 0.2394, Jaccard score = 0.1656, loss = 0.0073, val_loss 2.9416, eval: [6.4 s]
# Iteration 44 fit: [10.5 s]: Recall = 0.2426, Jaccard score = 0.1682, loss = 0.0070, val_loss 2.9524, eval: [6.3 s]
# Iteration 45 fit: [10.5 s]: Recall = 0.2403, Jaccard score = 0.1663, loss = 0.0065, val_loss 2.9685, eval: [6.3 s]
# Iteration 46 fit: [10.5 s]: Recall = 0.2447, Jaccard score = 0.1698, loss = 0.0061, val_loss 2.9872, eval: [6.3 s]
# Iteration 47 fit: [10.5 s]: Recall = 0.2504, Jaccard score = 0.1745, loss = 0.0057, val_loss 2.9969, eval: [6.3 s]
# Iteration 48 fit: [10.5 s]: Recall = 0.2522, Jaccard score = 0.1760, loss = 0.0055, val_loss 3.0119, eval: [6.3 s]
# Iteration 49 fit: [10.5 s]: Recall = 0.2541, Jaccard score = 0.1775, loss = 0.0053, val_loss 3.0174, eval: [6.3 s]
# Iteration 50 fit: [10.5 s]: Recall = 0.2526, Jaccard score = 0.1763, loss = 0.0052, val_loss 3.0313, eval: [6.3 s]
# Iteration 51 fit: [10.5 s]: Recall = 0.2569, Jaccard score = 0.1798, loss = 0.0050, val_loss 3.0365, eval: [6.3 s]
# Iteration 52 fit: [10.5 s]: Recall = 0.2570, Jaccard score = 0.1799, loss = 0.0048, val_loss 3.0525, eval: [6.3 s]
# Iteration 53 fit: [10.5 s]: Recall = 0.2606, Jaccard score = 0.1829, loss = 0.0045, val_loss 3.0512, eval: [6.3 s]
# Iteration 54 fit: [10.6 s]: Recall = 0.2626, Jaccard score = 0.1846, loss = 0.0044, val_loss 3.0576, eval: [6.4 s]
# Iteration 55 fit: [10.5 s]: Recall = 0.2643, Jaccard score = 0.1860, loss = 0.0043, val_loss 3.0702, eval: [6.4 s]
# Iteration 56 fit: [10.5 s]: Recall = 0.2694, Jaccard score = 0.1902, loss = 0.0041, val_loss 3.0667, eval: [6.4 s]
# Iteration 57 fit: [10.5 s]: Recall = 0.2708, Jaccard score = 0.1915, loss = 0.0041, val_loss 3.0769, eval: [6.5 s]
# Iteration 58 fit: [10.6 s]: Recall = 0.2728, Jaccard score = 0.1931, loss = 0.0040, val_loss 3.0829, eval: [6.4 s]
# Iteration 59 fit: [10.5 s]: Recall = 0.2755, Jaccard score = 0.1954, loss = 0.0039, val_loss 3.0877, eval: [6.4 s]
# Iteration 60 fit: [10.5 s]: Recall = 0.2772, Jaccard score = 0.1969, loss = 0.0037, val_loss 3.0898, eval: [6.4 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566306967.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566306967.h5
# Load data done [1.9 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0297, Jaccard score = 0.0179
# Iteration 0 fit: [12.8 s]: Recall = 0.0307, Jaccard score = 0.0186, loss = 0.5377, val_loss 0.5009, eval: [6.5 s]
# Iteration 1 fit: [10.5 s]: Recall = 0.0284, Jaccard score = 0.0172, loss = 0.5008, val_loss 0.5010, eval: [6.4 s]
# Iteration 2 fit: [10.6 s]: Recall = 0.0311, Jaccard score = 0.0188, loss = 0.4998, val_loss 0.5022, eval: [6.4 s]
# Iteration 3 fit: [10.5 s]: Recall = 0.0315, Jaccard score = 0.0191, loss = 0.4960, val_loss 0.5055, eval: [6.4 s]
# Iteration 4 fit: [10.5 s]: Recall = 0.0332, Jaccard score = 0.0201, loss = 0.4868, val_loss 0.5142, eval: [6.4 s]
# Iteration 5 fit: [10.6 s]: Recall = 0.0406, Jaccard score = 0.0247, loss = 0.4709, val_loss 0.5279, eval: [6.4 s]
# Iteration 6 fit: [10.5 s]: Recall = 0.0452, Jaccard score = 0.0276, loss = 0.4476, val_loss 0.5473, eval: [6.4 s]
# Iteration 7 fit: [10.8 s]: Recall = 0.0592, Jaccard score = 0.0364, loss = 0.4178, val_loss 0.5680, eval: [6.5 s]
# Iteration 8 fit: [10.7 s]: Recall = 0.0681, Jaccard score = 0.0421, loss = 0.3829, val_loss 0.5926, eval: [6.4 s]
# Iteration 9 fit: [10.5 s]: Recall = 0.0827, Jaccard score = 0.0516, loss = 0.3456, val_loss 0.6209, eval: [6.4 s]
# Iteration 10 fit: [10.6 s]: Recall = 0.0940, Jaccard score = 0.0591, loss = 0.3088, val_loss 0.6476, eval: [6.5 s]
# Iteration 11 fit: [10.5 s]: Recall = 0.1028, Jaccard score = 0.0650, loss = 0.2726, val_loss 0.6795, eval: [6.4 s]
# Iteration 12 fit: [10.7 s]: Recall = 0.1077, Jaccard score = 0.0683, loss = 0.2391, val_loss 0.7220, eval: [6.4 s]
# Iteration 13 fit: [10.6 s]: Recall = 0.1162, Jaccard score = 0.0740, loss = 0.2098, val_loss 0.7613, eval: [6.4 s]
# Iteration 14 fit: [10.6 s]: Recall = 0.1207, Jaccard score = 0.0772, loss = 0.1831, val_loss 0.8066, eval: [6.4 s]
# Iteration 15 fit: [10.6 s]: Recall = 0.1266, Jaccard score = 0.0812, loss = 0.1605, val_loss 0.8581, eval: [6.4 s]
# Iteration 16 fit: [10.6 s]: Recall = 0.1345, Jaccard score = 0.0867, loss = 0.1392, val_loss 0.9171, eval: [6.4 s]
# Iteration 17 fit: [10.6 s]: Recall = 0.1337, Jaccard score = 0.0861, loss = 0.1202, val_loss 0.9764, eval: [6.4 s]
# Iteration 18 fit: [10.6 s]: Recall = 0.1385, Jaccard score = 0.0895, loss = 0.1041, val_loss 1.0465, eval: [6.4 s]
# Iteration 19 fit: [10.6 s]: Recall = 0.1443, Jaccard score = 0.0936, loss = 0.0891, val_loss 1.1171, eval: [6.4 s]
# Iteration 20 fit: [10.6 s]: Recall = 0.1444, Jaccard score = 0.0937, loss = 0.0775, val_loss 1.1921, eval: [6.5 s]
# Iteration 21 fit: [10.5 s]: Recall = 0.1540, Jaccard score = 0.1006, loss = 0.0663, val_loss 1.2628, eval: [6.4 s]
# Iteration 22 fit: [10.6 s]: Recall = 0.1541, Jaccard score = 0.1006, loss = 0.0581, val_loss 1.3321, eval: [6.4 s]
# Iteration 23 fit: [10.6 s]: Recall = 0.1588, Jaccard score = 0.1040, loss = 0.0496, val_loss 1.4001, eval: [6.4 s]
# Iteration 24 fit: [10.6 s]: Recall = 0.1669, Jaccard score = 0.1099, loss = 0.0437, val_loss 1.4763, eval: [6.4 s]
# Iteration 25 fit: [10.6 s]: Recall = 0.1698, Jaccard score = 0.1120, loss = 0.0383, val_loss 1.5428, eval: [6.4 s]
# Iteration 26 fit: [10.6 s]: Recall = 0.1726, Jaccard score = 0.1141, loss = 0.0328, val_loss 1.6050, eval: [6.4 s]
# Iteration 27 fit: [10.6 s]: Recall = 0.1769, Jaccard score = 0.1173, loss = 0.0297, val_loss 1.6520, eval: [6.4 s]
# Iteration 28 fit: [10.5 s]: Recall = 0.1837, Jaccard score = 0.1223, loss = 0.0254, val_loss 1.7073, eval: [6.5 s]
# Iteration 29 fit: [10.6 s]: Recall = 0.1848, Jaccard score = 0.1232, loss = 0.0230, val_loss 1.7579, eval: [6.4 s]
# Iteration 30 fit: [10.6 s]: Recall = 0.1892, Jaccard score = 0.1264, loss = 0.0206, val_loss 1.8051, eval: [6.4 s]
# Iteration 31 fit: [10.5 s]: Recall = 0.1915, Jaccard score = 0.1282, loss = 0.0185, val_loss 1.8482, eval: [6.4 s]
# Iteration 32 fit: [10.5 s]: Recall = 0.2003, Jaccard score = 0.1348, loss = 0.0170, val_loss 1.8868, eval: [6.4 s]
# Iteration 33 fit: [10.5 s]: Recall = 0.2054, Jaccard score = 0.1388, loss = 0.0148, val_loss 1.9085, eval: [6.4 s]
# Iteration 34 fit: [10.5 s]: Recall = 0.2079, Jaccard score = 0.1407, loss = 0.0138, val_loss 1.9497, eval: [6.4 s]
# Iteration 35 fit: [10.5 s]: Recall = 0.2130, Jaccard score = 0.1447, loss = 0.0130, val_loss 1.9787, eval: [6.4 s]
# Iteration 36 fit: [10.5 s]: Recall = 0.2174, Jaccard score = 0.1481, loss = 0.0116, val_loss 2.0069, eval: [6.4 s]
# Iteration 37 fit: [10.5 s]: Recall = 0.2200, Jaccard score = 0.1501, loss = 0.0108, val_loss 2.0298, eval: [6.4 s]
# Iteration 38 fit: [11.5 s]: Recall = 0.2221, Jaccard score = 0.1518, loss = 0.0102, val_loss 2.0615, eval: [8.7 s]
# Iteration 39 fit: [12.8 s]: Recall = 0.2280, Jaccard score = 0.1565, loss = 0.0094, val_loss 2.0806, eval: [8.1 s]
# Iteration 40 fit: [10.9 s]: Recall = 0.2301, Jaccard score = 0.1581, loss = 0.0085, val_loss 2.0992, eval: [6.4 s]
# Iteration 41 fit: [10.5 s]: Recall = 0.2303, Jaccard score = 0.1583, loss = 0.0080, val_loss 2.1140, eval: [6.4 s]
# Iteration 42 fit: [10.5 s]: Recall = 0.2394, Jaccard score = 0.1656, loss = 0.0077, val_loss 2.1382, eval: [6.4 s]
# Iteration 43 fit: [10.6 s]: Recall = 0.2371, Jaccard score = 0.1637, loss = 0.0071, val_loss 2.1538, eval: [6.4 s]
# Iteration 44 fit: [10.5 s]: Recall = 0.2426, Jaccard score = 0.1682, loss = 0.0069, val_loss 2.1776, eval: [6.4 s]
# Iteration 45 fit: [10.7 s]: Recall = 0.2424, Jaccard score = 0.1680, loss = 0.0064, val_loss 2.1804, eval: [6.4 s]
# Iteration 46 fit: [10.6 s]: Recall = 0.2480, Jaccard score = 0.1725, loss = 0.0061, val_loss 2.1985, eval: [6.4 s]
# Iteration 47 fit: [10.5 s]: Recall = 0.2527, Jaccard score = 0.1764, loss = 0.0058, val_loss 2.2234, eval: [6.4 s]
# Iteration 48 fit: [10.5 s]: Recall = 0.2527, Jaccard score = 0.1764, loss = 0.0055, val_loss 2.2282, eval: [6.4 s]
# Iteration 49 fit: [10.5 s]: Recall = 0.2536, Jaccard score = 0.1771, loss = 0.0055, val_loss 2.2447, eval: [6.5 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566308003.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566308003.h5
# Load data done [1.9 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0270, Jaccard score = 0.0163
# Iteration 0 fit: [12.7 s]: Recall = 0.0281, Jaccard score = 0.0170, loss = 0.5379, val_loss 0.5010, eval: [6.4 s]
# Iteration 1 fit: [10.5 s]: Recall = 0.0280, Jaccard score = 0.0169, loss = 0.5009, val_loss 0.5012, eval: [6.4 s]
# Iteration 2 fit: [10.5 s]: Recall = 0.0294, Jaccard score = 0.0177, loss = 0.5000, val_loss 0.5017, eval: [6.4 s]
# Iteration 3 fit: [10.6 s]: Recall = 0.0318, Jaccard score = 0.0192, loss = 0.4961, val_loss 0.5055, eval: [6.4 s]
# Iteration 4 fit: [10.6 s]: Recall = 0.0335, Jaccard score = 0.0203, loss = 0.4864, val_loss 0.5147, eval: [6.4 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566308152.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566308152.h5
# Load data done [4.5 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0262, Jaccard score = 0.0158


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566308193.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566308193.h5


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566308204.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566308204.h5
# Load data done [4.4 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0257, Jaccard score = 0.0155
# Iteration 0 fit: [25.8 s]: Recall = 0.0286, Jaccard score = 0.0172, loss = 0.5374, val_loss 0.5008, eval: [9.9 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566312702.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566312702.h5
# Load data done [5.1 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0288, Jaccard score = 0.0174
# Iteration 0 fit: [26.3 s]: Recall = 0.0294, Jaccard score = 0.0177, loss = 0.5374, val_loss 0.5009, eval: [9.8 s]
# Iteration 1 fit: [19.5 s]: Recall = 0.0259, Jaccard score = 0.0156, loss = 0.5009, val_loss 0.5013, eval: [9.9 s]
# Iteration 2 fit: [20.7 s]: Recall = 0.0265, Jaccard score = 0.0160, loss = 0.4999, val_loss 0.5021, eval: [9.8 s]
# Iteration 3 fit: [19.3 s]: Recall = 0.0287, Jaccard score = 0.0173, loss = 0.4964, val_loss 0.5055, eval: [9.8 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566319572.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566319572.h5
# Load data done [4.4 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566319625.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566319625.h5
# Load data done [4.4 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0311, Jaccard score = 0.0188
# Iteration 0 fit: [25.7 s]: Recall = 0.0270, Jaccard score = 0.0163, loss = 0.5380, val_loss 0.5012, eval: [9.8 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566323080.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566323080.h5
# Load data done [1.6 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0312, Jaccard score = 0.0189
# Iteration 0 fit: [12.6 s]: Recall = 0.0288, Jaccard score = 0.0174, loss = 0.5380, val_loss 1.5972, eval: [6.4 s]
# Iteration 1 fit: [10.4 s]: Recall = 0.0274, Jaccard score = 0.0165, loss = 0.5008, val_loss 1.6140, eval: [6.4 s]
# Iteration 2 fit: [10.4 s]: Recall = 0.0290, Jaccard score = 0.0175, loss = 0.4997, val_loss 1.6300, eval: [6.4 s]
# Iteration 3 fit: [10.4 s]: Recall = 0.0290, Jaccard score = 0.0175, loss = 0.4954, val_loss 1.6357, eval: [6.4 s]
# Iteration 4 fit: [10.4 s]: Recall = 0.0329, Jaccard score = 0.0199, loss = 0.4859, val_loss 1.7085, eval: [6.4 s]
# Iteration 5 fit: [10.4 s]: Recall = 0.0390, Jaccard score = 0.0237, loss = 0.4692, val_loss 1.7934, eval: [6.4 s]
# Iteration 6 fit: [10.4 s]: Recall = 0.0465, Jaccard score = 0.0284, loss = 0.4451, val_loss 1.9030, eval: [6.4 s]
# Iteration 7 fit: [10.4 s]: Recall = 0.0567, Jaccard score = 0.0348, loss = 0.4145, val_loss 2.0838, eval: [6.4 s]
# Iteration 8 fit: [10.4 s]: Recall = 0.0681, Jaccard score = 0.0421, loss = 0.3781, val_loss 2.2565, eval: [6.4 s]
# Iteration 9 fit: [10.4 s]: Recall = 0.0781, Jaccard score = 0.0486, loss = 0.3400, val_loss 2.4387, eval: [6.4 s]
# Iteration 10 fit: [10.4 s]: Recall = 0.0906, Jaccard score = 0.0568, loss = 0.3027, val_loss 2.6340, eval: [6.4 s]
# Iteration 11 fit: [10.5 s]: Recall = 0.0960, Jaccard score = 0.0604, loss = 0.2675, val_loss 2.8508, eval: [6.5 s]
# Iteration 12 fit: [10.4 s]: Recall = 0.1070, Jaccard score = 0.0678, loss = 0.2360, val_loss 3.0871, eval: [6.4 s]
# Iteration 13 fit: [10.5 s]: Recall = 0.1130, Jaccard score = 0.0718, loss = 0.2064, val_loss 3.3465, eval: [6.3 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566323346.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566323346.h5
# Load data done [4.4 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0281, Jaccard score = 0.0170
# Iteration 0 fit: [25.0 s]: Recall = 0.0302, Jaccard score = 0.0182, loss = 0.5372, val_loss 1.6021, eval: [9.8 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566376856.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566376856.h5
# Load data done [1.9 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0262, Jaccard score = 0.0158
# Iteration 0 fit: [15.1 s]: Recall = 0.0279, Jaccard score = 0.0168, loss = 0.5381, val_loss 0.5008, eval: [6.4 s]
# Iteration 1 fit: [12.5 s]: Recall = 0.0283, Jaccard score = 0.0171, loss = 0.5008, val_loss 0.4995, eval: [6.3 s]
# Iteration 2 fit: [12.5 s]: Recall = 0.0312, Jaccard score = 0.0189, loss = 0.4996, val_loss 0.4956, eval: [6.4 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566376970.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566376970.h5
# Load data done [1.9 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0271, Jaccard score = 0.0163


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566376994.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566376994.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566377015.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566377015.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0274, Jaccard score = 0.0165
# Iteration 0 fit: [13.0 s]: Recall = 0.0287, Jaccard score = 0.0173, loss = 0.5383, val_loss 1.5902, eval: [6.4 s]
# Iteration 1 fit: [10.8 s]: Recall = 0.0240, Jaccard score = 0.0144, loss = 0.5008, val_loss 1.6153, eval: [6.3 s]
# Iteration 2 fit: [10.9 s]: Recall = 0.0302, Jaccard score = 0.0182, loss = 0.5000, val_loss 1.5978, eval: [6.4 s]
# Iteration 3 fit: [10.9 s]: Recall = 0.0318, Jaccard score = 0.0192, loss = 0.4963, val_loss 1.5598, eval: [6.4 s]
# Iteration 4 fit: [10.9 s]: Recall = 0.0339, Jaccard score = 0.0205, loss = 0.4874, val_loss 1.5076, eval: [6.4 s]
# Iteration 5 fit: [10.8 s]: Recall = 0.0376, Jaccard score = 0.0228, loss = 0.4716, val_loss 1.4015, eval: [6.3 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566377278.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566377278.h5
# Load data done [1.9 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0310, Jaccard score = 0.0187
# Iteration 0 fit: [12.6 s]: Recall = 0.0306, Jaccard score = 0.0185, loss = 0.5375, val_loss 1.5954, eval: [6.4 s]
# Iteration 1 fit: [10.4 s]: Recall = 0.0295, Jaccard score = 0.0178, loss = 0.5008, val_loss 1.6093, eval: [6.4 s]
# Iteration 2 fit: [10.4 s]: Recall = 0.0334, Jaccard score = 0.0202, loss = 0.5000, val_loss 1.5924, eval: [6.3 s]
# Iteration 3 fit: [10.5 s]: Recall = 0.0326, Jaccard score = 0.0197, loss = 0.4961, val_loss 1.5418, eval: [6.4 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566377376.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566377376.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0290, Jaccard score = 0.0175
# Iteration 0 fit: [12.6 s]: Recall = 0.0291, Jaccard score = 0.0176, loss = 0.5377, val_loss 1.5998, eval: [6.4 s]
# Iteration 1 fit: [10.4 s]: Recall = 0.0271, Jaccard score = 0.0163, loss = 0.5009, val_loss 1.6121, eval: [6.3 s]
# Iteration 2 fit: [10.4 s]: Recall = 0.0305, Jaccard score = 0.0184, loss = 0.5002, val_loss 1.6074, eval: [6.3 s]
# Iteration 3 fit: [10.3 s]: Recall = 0.0330, Jaccard score = 0.0200, loss = 0.4970, val_loss 1.6256, eval: [6.3 s]
# Iteration 4 fit: [10.4 s]: Recall = 0.0352, Jaccard score = 0.0213, loss = 0.4888, val_loss 1.6841, eval: [6.3 s]
# Iteration 5 fit: [10.4 s]: Recall = 0.0376, Jaccard score = 0.0228, loss = 0.4740, val_loss 1.7687, eval: [6.3 s]
# Iteration 6 fit: [10.4 s]: Recall = 0.0485, Jaccard score = 0.0297, loss = 0.4518, val_loss 1.8852, eval: [6.3 s]
# Iteration 7 fit: [10.4 s]: Recall = 0.0555, Jaccard score = 0.0341, loss = 0.4225, val_loss 2.0450, eval: [6.3 s]
# Iteration 8 fit: [10.4 s]: Recall = 0.0650, Jaccard score = 0.0401, loss = 0.3876, val_loss 2.2371, eval: [6.3 s]
# Iteration 9 fit: [10.4 s]: Recall = 0.0764, Jaccard score = 0.0475, loss = 0.3505, val_loss 2.4168, eval: [6.3 s]
# Iteration 10 fit: [10.3 s]: Recall = 0.0870, Jaccard score = 0.0545, loss = 0.3124, val_loss 2.6357, eval: [6.3 s]
# Iteration 11 fit: [10.4 s]: Recall = 0.0960, Jaccard score = 0.0604, loss = 0.2759, val_loss 2.8585, eval: [6.3 s]
# Iteration 12 fit: [10.3 s]: Recall = 0.1030, Jaccard score = 0.0651, loss = 0.2420, val_loss 3.1076, eval: [6.3 s]
# Iteration 13 fit: [10.4 s]: Recall = 0.1099, Jaccard score = 0.0698, loss = 0.2126, val_loss 3.3650, eval: [6.3 s]
# Iteration 14 fit: [10.4 s]: Recall = 0.1227, Jaccard score = 0.0785, loss = 0.1850, val_loss 3.6373, eval: [6.3 s]
# Iteration 15 fit: [10.4 s]: Recall = 0.1220, Jaccard score = 0.0780, loss = 0.1612, val_loss 3.9353, eval: [6.3 s]
# Iteration 16 fit: [10.3 s]: Recall = 0.1273, Jaccard score = 0.0817, loss = 0.1395, val_loss 4.2718, eval: [6.3 s]
# Iteration 17 fit: [10.4 s]: Recall = 0.1265, Jaccard score = 0.0811, loss = 0.1208, val_loss 4.6226, eval: [6.3 s]
# Iteration 18 fit: [10.3 s]: Recall = 0.1380, Jaccard score = 0.0892, loss = 0.1041, val_loss 4.9999, eval: [6.3 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566378779.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566378779.h5
# Load data done [4.4 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0304, Jaccard score = 0.0184


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.5, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566390957.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566390957.h5
# Load data done [2.0 s]. #user=20000, #item=2000, #train=161729, #test=eval_recall
# 
# Performing k-fold 1
# Init: Recall = 0.0303, Jaccard score = 0.0183
# Iteration 0 fit: [17.9 s]: Recall = 0.0282, Jaccard score = 0.0170, loss = 0.5073, eval: [7.9 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566391017.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566391017.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# 
# Performing k-fold 1


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.1_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.1, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566391033.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566391033.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=147903, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 128)          128128      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 128)          512         feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 222)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            223         concatenate_1[0][0]              
# ==================================================================================================
# Total params: 2,196,863
# Trainable params: 2,196,607
# Non-trainable params: 256
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0267, Jaccard score = 0.0197
# Iteration 0 fit: [16.4 s]: Recall = 0.0283, Jaccard score = 0.0208, loss = 0.5067, eval: [8.4 s]
# Iteration 1 fit: [15.9 s]: Recall = 0.0278, Jaccard score = 0.0205, loss = 0.5024, eval: [8.4 s]
# Iteration 2 fit: [15.8 s]: Recall = 0.0284, Jaccard score = 0.0210, loss = 0.5006, eval: [8.4 s]
# Iteration 3 fit: [15.8 s]: Recall = 0.0300, Jaccard score = 0.0222, loss = 0.4961, eval: [8.5 s]
# Iteration 4 fit: [15.8 s]: Recall = 0.0350, Jaccard score = 0.0259, loss = 0.4859, eval: [8.4 s]
# Iteration 5 fit: [15.9 s]: Recall = 0.0369, Jaccard score = 0.0274, loss = 0.4691, eval: [8.5 s]
# Iteration 6 fit: [15.9 s]: Recall = 0.0422, Jaccard score = 0.0315, loss = 0.4452, eval: [8.6 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566391234.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566391234.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 128)          128128      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 128)          512         feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 222)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            223         concatenate_1[0][0]              
# ==================================================================================================
# Total params: 2,196,863
# Trainable params: 2,196,607
# Non-trainable params: 256
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0272, Jaccard score = 0.0208
# Iteration 0 fit: [15.9 s]: Recall = 0.0288, Jaccard score = 0.0220, loss = 0.5076, eval: [7.3 s]
# Iteration 1 fit: [15.4 s]: Recall = 0.0289, Jaccard score = 0.0221, loss = 0.5026, eval: [7.3 s]
# Iteration 2 fit: [15.3 s]: Recall = 0.0284, Jaccard score = 0.0217, loss = 0.5007, eval: [7.2 s]
# Iteration 3 fit: [15.4 s]: Recall = 0.0282, Jaccard score = 0.0215, loss = 0.4956, eval: [7.3 s]
# Iteration 4 fit: [15.3 s]: Recall = 0.0300, Jaccard score = 0.0230, loss = 0.4850, eval: [7.2 s]
# Iteration 5 fit: [15.4 s]: Recall = 0.0322, Jaccard score = 0.0247, loss = 0.4680, eval: [7.3 s]
# Iteration 6 fit: [15.4 s]: Recall = 0.0340, Jaccard score = 0.0261, loss = 0.4438, eval: [7.1 s]
# Iteration 7 fit: [15.3 s]: Recall = 0.0365, Jaccard score = 0.0280, loss = 0.4138, eval: [7.2 s]
# Iteration 8 fit: [15.3 s]: Recall = 0.0393, Jaccard score = 0.0303, loss = 0.3787, eval: [7.4 s]
# Iteration 9 fit: [15.5 s]: Recall = 0.0420, Jaccard score = 0.0324, loss = 0.3417, eval: [7.2 s]
# Iteration 10 fit: [15.6 s]: Recall = 0.0443, Jaccard score = 0.0343, loss = 0.3045, eval: [7.5 s]
# Iteration 11 fit: [17.0 s]: Recall = 0.0449, Jaccard score = 0.0348, loss = 0.2701, eval: [7.5 s]
# Iteration 12 fit: [15.9 s]: Recall = 0.0465, Jaccard score = 0.0360, loss = 0.2366, eval: [7.2 s]
# Iteration 13 fit: [17.8 s]: Recall = 0.0473, Jaccard score = 0.0367, loss = 0.2075, eval: [8.0 s]
# Iteration 14 fit: [15.9 s]: Recall = 0.0472, Jaccard score = 0.0366, loss = 0.1815, eval: [8.0 s]
# Iteration 15 fit: [18.4 s]: Recall = 0.0470, Jaccard score = 0.0365, loss = 0.1575, eval: [9.4 s]
# Iteration 16 fit: [15.4 s]: Recall = 0.0457, Jaccard score = 0.0354, loss = 0.1365, eval: [7.2 s]
# Iteration 17 fit: [15.4 s]: Recall = 0.0448, Jaccard score = 0.0346, loss = 0.1180, eval: [7.1 s]
# Iteration 18 fit: [15.4 s]: Recall = 0.0450, Jaccard score = 0.0348, loss = 0.1014, eval: [7.3 s]
# Iteration 19 fit: [15.3 s]: Recall = 0.0450, Jaccard score = 0.0348, loss = 0.0868, eval: [7.1 s]
# Iteration 20 fit: [15.4 s]: Recall = 0.0443, Jaccard score = 0.0343, loss = 0.0741, eval: [7.2 s]
# Iteration 21 fit: [15.4 s]: Recall = 0.0430, Jaccard score = 0.0332, loss = 0.0638, eval: [7.2 s]
# Iteration 22 fit: [15.3 s]: Recall = 0.0405, Jaccard score = 0.0312, loss = 0.0548, eval: [7.3 s]
# Iteration 23 fit: [15.4 s]: Recall = 0.0407, Jaccard score = 0.0314, loss = 0.0479, eval: [7.2 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566391817.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566391817.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 128)          128128      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 128)          512         feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 222)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 128)          28544       concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 128)          512         final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 128)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            129         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,225,825
# Trainable params: 2,225,313
# Non-trainable params: 512
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0245, Jaccard score = 0.0187
# Iteration 0 fit: [18.6 s]: Recall = 0.1033, Jaccard score = 0.0837, loss = 0.4999, eval: [7.9 s]
# Iteration 1 fit: [18.2 s]: Recall = 0.1302, Jaccard score = 0.1079, loss = 0.4464, eval: [7.9 s]
# Iteration 2 fit: [18.2 s]: Recall = 0.1293, Jaccard score = 0.1071, loss = 0.4136, eval: [7.9 s]
# Iteration 3 fit: [18.2 s]: Recall = 0.1346, Jaccard score = 0.1119, loss = 0.3937, eval: [7.9 s]
# Iteration 4 fit: [18.1 s]: Recall = 0.1270, Jaccard score = 0.1050, loss = 0.3744, eval: [7.9 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566391990.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566391990.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 128)          128128      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 128)          512         feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 222)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          57088       concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,255,009
# Trainable params: 2,254,241
# Non-trainable params: 768
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0279, Jaccard score = 0.0213
# Iteration 0 fit: [18.8 s]: Recall = 0.1135, Jaccard score = 0.0927, loss = 0.4958, eval: [7.9 s]
# Iteration 1 fit: [18.1 s]: Recall = 0.1310, Jaccard score = 0.1086, loss = 0.4404, eval: [7.9 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566392097.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566392097.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 128)          128128      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 128)          512         feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 128)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 222)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_2[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          57088       concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_3 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_3[0][0]              
# ==================================================================================================
# Total params: 2,255,009
# Trainable params: 2,254,241
# Non-trainable params: 768
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0263, Jaccard score = 0.0201
# Iteration 0 fit: [18.5 s]: Recall = 0.1123, Jaccard score = 0.0917, loss = 0.4970, eval: [7.8 s]
# Iteration 1 fit: [17.7 s]: Recall = 0.1329, Jaccard score = 0.1103, loss = 0.4422, eval: [7.7 s]
# Iteration 2 fit: [17.7 s]: Recall = 0.1341, Jaccard score = 0.1114, loss = 0.4134, eval: [7.8 s]
# Iteration 3 fit: [17.7 s]: Recall = 0.1320, Jaccard score = 0.1095, loss = 0.3936, eval: [7.7 s]
# Iteration 4 fit: [17.7 s]: Recall = 0.1310, Jaccard score = 0.1086, loss = 0.3743, eval: [7.8 s]
# Iteration 5 fit: [17.7 s]: Recall = 0.1252, Jaccard score = 0.1033, loss = 0.3513, eval: [7.6 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566392298.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566392298.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 512)          512512      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 512)          2048        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 606)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          155392      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,739,233
# Trainable params: 2,737,697
# Non-trainable params: 1,536
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0267, Jaccard score = 0.0204
# Iteration 0 fit: [19.7 s]: Recall = 0.0267, Jaccard score = 0.0204, loss = 0.5059, eval: [7.7 s]
# Iteration 1 fit: [18.8 s]: Recall = 0.1319, Jaccard score = 0.1094, loss = 0.4846, eval: [7.7 s]
# Iteration 2 fit: [19.3 s]: Recall = 0.1352, Jaccard score = 0.1125, loss = 0.4296, eval: [7.8 s]
# Iteration 3 fit: [19.4 s]: Recall = 0.1325, Jaccard score = 0.1100, loss = 0.4054, eval: [7.8 s]
# Iteration 4 fit: [19.2 s]: Recall = 0.1287, Jaccard score = 0.1065, loss = 0.3866, eval: [7.7 s]
# Iteration 5 fit: [19.3 s]: Recall = 0.1280, Jaccard score = 0.1059, loss = 0.3656, eval: [7.8 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566392515.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566392515.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566392529.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566392529.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1094)         0           multiply_1[0][0]                 
#                                                                  user_feature_input[0][0]         
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          280320      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_1[0][0]              
# ==================================================================================================
# Total params: 2,349,601
# Trainable params: 2,349,089
# Non-trainable params: 512
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0268, Jaccard score = 0.0205
# Iteration 0 fit: [18.2 s]: Recall = 0.1185, Jaccard score = 0.0972, loss = 0.4951, eval: [7.4 s]
# Iteration 1 fit: [17.2 s]: Recall = 0.1304, Jaccard score = 0.1081, loss = 0.4418, eval: [7.4 s]
# Iteration 2 fit: [16.7 s]: Recall = 0.1329, Jaccard score = 0.1103, loss = 0.4120, eval: [7.4 s]
# Iteration 3 fit: [16.7 s]: Recall = 0.1327, Jaccard score = 0.1102, loss = 0.3925, eval: [7.3 s]
# Iteration 4 fit: [17.2 s]: Recall = 0.1314, Jaccard score = 0.1090, loss = 0.3739, eval: [7.4 s]
# Iteration 5 fit: [16.8 s]: Recall = 0.1277, Jaccard score = 0.1056, loss = 0.3532, eval: [7.3 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566392698.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566392698.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 512)          512512      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 512)          2048        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 606)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 512)          310784      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 512)          2048        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 512)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            513         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,895,905
# Trainable params: 2,893,857
# Non-trainable params: 2,048
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0273, Jaccard score = 0.0208
# Iteration 0 fit: [20.2 s]: Recall = 0.0570, Jaccard score = 0.0445, loss = 0.5058, eval: [7.7 s]
# Iteration 1 fit: [19.3 s]: Recall = 0.1313, Jaccard score = 0.1089, loss = 0.4568, eval: [7.9 s]
# Iteration 2 fit: [19.3 s]: Recall = 0.1337, Jaccard score = 0.1111, loss = 0.4176, eval: [7.8 s]
# Iteration 3 fit: [19.3 s]: Recall = 0.1337, Jaccard score = 0.1111, loss = 0.3986, eval: [7.9 s]
# Iteration 4 fit: [19.3 s]: Recall = 0.1339, Jaccard score = 0.1113, loss = 0.3791, eval: [7.9 s]
# Iteration 5 fit: [19.3 s]: Recall = 0.1276, Jaccard score = 0.1055, loss = 0.3558, eval: [7.8 s]
# Iteration 6 fit: [19.3 s]: Recall = 0.1228, Jaccard score = 0.1011, loss = 0.3272, eval: [7.9 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566393657.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566393657.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 350)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 96)           33696       concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 96)           384         final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,359,457
# Trainable params: 2,358,753
# Non-trainable params: 704
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0285, Jaccard score = 0.0218


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566393679.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566393679.h5
# Load data done [1.5 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 350)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 96)           33696       concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 96)           384         final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,359,457
# Trainable params: 2,358,753
# Non-trainable params: 704
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0261, Jaccard score = 0.0199
# Iteration 0 fit: [19.2 s]: Recall = 0.0549, Jaccard score = 0.0429, loss = 0.5035, eval: [8.0 s]
# Iteration 1 fit: [20.5 s]: Recall = 0.1299, Jaccard score = 0.1076, loss = 0.4609, eval: [7.7 s]
# Iteration 2 fit: [18.5 s]: Recall = 0.1359, Jaccard score = 0.1131, loss = 0.4197, eval: [7.9 s]
# Iteration 3 fit: [18.5 s]: Recall = 0.1338, Jaccard score = 0.1112, loss = 0.3986, eval: [7.8 s]
# Iteration 4 fit: [18.5 s]: Recall = 0.1304, Jaccard score = 0.1081, loss = 0.3791, eval: [7.8 s]
# Iteration 5 fit: [18.8 s]: Recall = 0.1295, Jaccard score = 0.1072, loss = 0.3581, eval: [7.8 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566393858.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566393858.h5
# Load data done [1.5 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 128)          128128      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 128)          512         feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 222)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 96)           21408       concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 96)           384         final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,218,529
# Trainable params: 2,218,081
# Non-trainable params: 448
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0297, Jaccard score = 0.0227
# Iteration 0 fit: [18.5 s]: Recall = 0.0981, Jaccard score = 0.0792, loss = 0.4991, eval: [7.8 s]
# Iteration 1 fit: [17.7 s]: Recall = 0.1320, Jaccard score = 0.1096, loss = 0.4475, eval: [7.7 s]
# Iteration 2 fit: [17.7 s]: Recall = 0.1334, Jaccard score = 0.1109, loss = 0.4134, eval: [7.8 s]
# Iteration 3 fit: [17.7 s]: Recall = 0.1324, Jaccard score = 0.1099, loss = 0.3938, eval: [7.9 s]
# Iteration 4 fit: [17.7 s]: Recall = 0.1289, Jaccard score = 0.1067, loss = 0.3743, eval: [7.7 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566394040.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566394040.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# user_feature_input (InputLay (None, 1000)              0         
# _________________________________________________________________
# feature_dense_layer (Dense)  (None, 512)               512512    
# _________________________________________________________________
# feature_dense_layer_bn (Batc (None, 512)               2048      
# _________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         
# _________________________________________________________________
# prediction (Dense)           (None, 1)                 513       
# =================================================================
# Total params: 515,073
# Trainable params: 514,049
# Non-trainable params: 1,024
# _________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0295, Jaccard score = 0.0226
# Iteration 0 fit: [12.5 s]: Recall = 0.0285, Jaccard score = 0.0218, loss = 0.5075, eval: [6.8 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566394114.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566394114.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 512)          512512      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 512)          2048        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 606)          0           flatten_2[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            607         concatenate_1[0][0]              
# ==================================================================================================
# Total params: 703,167
# Trainable params: 702,143
# Non-trainable params: 1,024
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0284, Jaccard score = 0.0217
# Iteration 0 fit: [13.0 s]: Recall = 0.1379, Jaccard score = 0.1150, loss = 0.4442, eval: [7.0 s]
# Iteration 1 fit: [12.6 s]: Recall = 0.1379, Jaccard score = 0.1150, loss = 0.4233, eval: [7.0 s]
# Iteration 2 fit: [12.5 s]: Recall = 0.1353, Jaccard score = 0.1126, loss = 0.4214, eval: [7.0 s]
# Iteration 3 fit: [13.3 s]: Recall = 0.1380, Jaccard score = 0.1151, loss = 0.4209, eval: [7.1 s]
# Iteration 4 fit: [12.7 s]: Recall = 0.1389, Jaccard score = 0.1160, loss = 0.4207, eval: [7.8 s]
# Iteration 5 fit: [14.6 s]: Recall = 0.1362, Jaccard score = 0.1134, loss = 0.4205, eval: [7.1 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566394256.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566394256.h5
# Load data done [1.5 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 512)          512512      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 512)          2048        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 606)          0           flatten_2[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 96)           58272       concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 96)           384         final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 761,313
# Trainable params: 760,097
# Non-trainable params: 1,216
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0220, Jaccard score = 0.0167
# Iteration 0 fit: [15.5 s]: Recall = 0.1693, Jaccard score = 0.1450, loss = 0.4336, eval: [7.6 s]
# Iteration 1 fit: [14.8 s]: Recall = 0.1861, Jaccard score = 0.1617, loss = 0.3897, eval: [7.7 s]
# Iteration 2 fit: [14.8 s]: Recall = 0.1942, Jaccard score = 0.1699, loss = 0.3699, eval: [7.7 s]
# Iteration 3 fit: [14.7 s]: Recall = 0.1991, Jaccard score = 0.1749, loss = 0.3562, eval: [7.6 s]
# Iteration 4 fit: [14.8 s]: Recall = 0.2003, Jaccard score = 0.1762, loss = 0.3445, eval: [7.6 s]
# Iteration 5 fit: [14.8 s]: Recall = 0.2004, Jaccard score = 0.1763, loss = 0.3335, eval: [7.7 s]
# Iteration 6 fit: [14.7 s]: Recall = 0.1997, Jaccard score = 0.1755, loss = 0.3239, eval: [7.7 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566462486.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566462486.h5
# Load data done [1.8 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 512)          512512      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 512)          2048        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 606)          0           flatten_2[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          155392      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 859,233
# Trainable params: 857,697
# Non-trainable params: 1,536
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0253, Jaccard score = 0.0193
# Iteration 0 fit: [17.0 s]: Recall = 0.1743, Jaccard score = 0.1499, loss = 0.4330, eval: [7.5 s]
# Iteration 1 fit: [15.4 s]: Recall = 0.1927, Jaccard score = 0.1683, loss = 0.3878, eval: [7.6 s]
# Iteration 2 fit: [15.3 s]: Recall = 0.2012, Jaccard score = 0.1772, loss = 0.3659, eval: [7.6 s]
# Iteration 3 fit: [15.2 s]: Recall = 0.2005, Jaccard score = 0.1764, loss = 0.3505, eval: [7.6 s]
# Iteration 4 fit: [15.2 s]: Recall = 0.2034, Jaccard score = 0.1795, loss = 0.3376, eval: [7.6 s]
# Iteration 5 fit: [15.3 s]: Recall = 0.2024, Jaccard score = 0.1784, loss = 0.3250, eval: [7.6 s]
# Iteration 6 fit: [15.3 s]: Recall = 0.2013, Jaccard score = 0.1772, loss = 0.3134, eval: [7.6 s]
# Iteration 7 fit: [15.3 s]: Recall = 0.2014, Jaccard score = 0.1774, loss = 0.3022, eval: [7.5 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566462709.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566462709.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 256)          256256      user_feature_input[0][0]         
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 256)          1024        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 350)          0           flatten_2[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          89856       concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 536,417
# Trainable params: 535,393
# Non-trainable params: 1,024
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0254, Jaccard score = 0.0194
# Iteration 0 fit: [14.9 s]: Recall = 0.1841, Jaccard score = 0.1597, loss = 0.4262, eval: [7.5 s]
# Iteration 1 fit: [14.2 s]: Recall = 0.1938, Jaccard score = 0.1695, loss = 0.3835, eval: [7.5 s]
# Iteration 2 fit: [14.2 s]: Recall = 0.1976, Jaccard score = 0.1734, loss = 0.3632, eval: [7.5 s]
# Iteration 3 fit: [14.2 s]: Recall = 0.2010, Jaccard score = 0.1769, loss = 0.3492, eval: [7.5 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566462834.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566462834.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1094)         0           flatten_2[0][0]                  
#                                                                  user_feature_input[0][0]         
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          280320      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_1[0][0]              
# ==================================================================================================
# Total params: 469,601
# Trainable params: 469,089
# Non-trainable params: 512
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0249, Jaccard score = 0.0190
# Iteration 0 fit: [13.6 s]: Recall = 0.1759, Jaccard score = 0.1515, loss = 0.4274, eval: [7.3 s]
# Iteration 1 fit: [13.2 s]: Recall = 0.1906, Jaccard score = 0.1662, loss = 0.3913, eval: [7.3 s]
# Iteration 2 fit: [13.2 s]: Recall = 0.1966, Jaccard score = 0.1724, loss = 0.3724, eval: [7.3 s]
# Iteration 3 fit: [13.1 s]: Recall = 0.2024, Jaccard score = 0.1783, loss = 0.3590, eval: [7.3 s]
# Iteration 4 fit: [13.2 s]: Recall = 0.2051, Jaccard score = 0.1812, loss = 0.3490, eval: [7.3 s]
# Iteration 5 fit: [13.1 s]: Recall = 0.2056, Jaccard score = 0.1817, loss = 0.3407, eval: [7.3 s]
# Iteration 6 fit: [13.0 s]: Recall = 0.2065, Jaccard score = 0.1826, loss = 0.3341, eval: [7.3 s]
# Iteration 7 fit: [13.2 s]: Recall = 0.2077, Jaccard score = 0.1839, loss = 0.3270, eval: [7.3 s]
# Iteration 8 fit: [13.2 s]: Recall = 0.2065, Jaccard score = 0.1826, loss = 0.3206, eval: [7.3 s]
# Iteration 9 fit: [13.1 s]: Recall = 0.2094, Jaccard score = 0.1857, loss = 0.3148, eval: [7.4 s]
# Iteration 10 fit: [13.2 s]: Recall = 0.2071, Jaccard score = 0.1833, loss = 0.3098, eval: [7.2 s]
# Iteration 11 fit: [13.2 s]: Recall = 0.2073, Jaccard score = 0.1835, loss = 0.3041, eval: [7.3 s]
# Iteration 12 fit: [13.2 s]: Recall = 0.2062, Jaccard score = 0.1823, loss = 0.2987, eval: [7.3 s]
# Iteration 13 fit: [13.2 s]: Recall = 0.2061, Jaccard score = 0.1823, loss = 0.2940, eval: [7.3 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566463164.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566463164.h5
# Load data done [1.6 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# feature_dense_layer (Dense)     (None, 1000)         1001000     user_feature_input[0][0]         
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# feature_dense_layer_bn (BatchNo (None, 1000)         4000        feature_dense_layer[0][0]        
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 1000)         0           feature_dense_layer_bn[0][0]     
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1094)         0           flatten_2[0][0]                  
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          280320      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 1,474,601
# Trainable params: 1,472,089
# Non-trainable params: 2,512
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0212, Jaccard score = 0.0161
# Iteration 0 fit: [17.8 s]: Recall = 0.1672, Jaccard score = 0.1429, loss = 0.4365, eval: [7.7 s]
# Iteration 1 fit: [17.1 s]: Recall = 0.1834, Jaccard score = 0.1590, loss = 0.3936, eval: [7.7 s]
# Iteration 2 fit: [17.1 s]: Recall = 0.1917, Jaccard score = 0.1674, loss = 0.3745, eval: [7.7 s]
# Iteration 3 fit: [17.1 s]: Recall = 0.1975, Jaccard score = 0.1733, loss = 0.3603, eval: [7.7 s]
# Iteration 4 fit: [17.1 s]: Recall = 0.1979, Jaccard score = 0.1738, loss = 0.3476, eval: [7.7 s]
# Iteration 5 fit: [17.1 s]: Recall = 0.2036, Jaccard score = 0.1796, loss = 0.3354, eval: [7.7 s]
# Iteration 6 fit: [17.1 s]: Recall = 0.2036, Jaccard score = 0.1796, loss = 0.3232, eval: [7.7 s]
# Iteration 7 fit: [17.1 s]: Recall = 0.1996, Jaccard score = 0.1755, loss = 0.3119, eval: [7.7 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566463428.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566463428.h5
# Load data done [2.0 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1188)         0           multiply_1[0][0]                 
#                                                                  flatten_2[0][0]                  
#                                                                  user_feature_input[0][0]         
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          304384      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_1[0][0]              
# ==================================================================================================
# Total params: 2,373,665
# Trainable params: 2,373,153
# Non-trainable params: 512
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0294, Jaccard score = 0.0225
# Iteration 0 fit: [17.3 s]: Recall = 0.1614, Jaccard score = 0.1372, loss = 0.4382, eval: [7.3 s]
# Iteration 1 fit: [16.7 s]: Recall = 0.1773, Jaccard score = 0.1528, loss = 0.3999, eval: [7.4 s]
# Iteration 2 fit: [16.7 s]: Recall = 0.1858, Jaccard score = 0.1614, loss = 0.3796, eval: [7.4 s]
# Iteration 3 fit: [16.7 s]: Recall = 0.1907, Jaccard score = 0.1663, loss = 0.3622, eval: [7.3 s]
# Iteration 4 fit: [16.7 s]: Recall = 0.1913, Jaccard score = 0.1670, loss = 0.3461, eval: [7.4 s]
# Iteration 5 fit: [16.7 s]: Recall = 0.1901, Jaccard score = 0.1657, loss = 0.3288, eval: [7.4 s]
# Iteration 6 fit: [16.7 s]: Recall = 0.1879, Jaccard score = 0.1635, loss = 0.3106, eval: [7.3 s]
# Iteration 7 fit: [16.7 s]: Recall = 0.1829, Jaccard score = 0.1585, loss = 0.2910, eval: [7.4 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566463864.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566463864.h5
# Load data done [1.6 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566463880.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566463880.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1094)         0           flatten_2[0][0]                  
#                                                                  user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# user_feature_item_latent (Dense (None, 256)          280320      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_item_latent_bn (Ba (None, 256)          1024        user_feature_item_latent[0][0]   
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 256)          0           user_feature_item_latent_bn[0][0]
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 350)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          89856       concatenate_2[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,440,481
# Trainable params: 2,439,457
# Non-trainable params: 1,024
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0265, Jaccard score = 0.0202
# Iteration 0 fit: [19.6 s]: Recall = 0.1766, Jaccard score = 0.1522, loss = 0.4254, eval: [7.8 s]
# Iteration 1 fit: [19.1 s]: Recall = 0.1894, Jaccard score = 0.1650, loss = 0.3855, eval: [7.9 s]
# Iteration 2 fit: [19.1 s]: Recall = 0.1936, Jaccard score = 0.1693, loss = 0.3642, eval: [7.9 s]
# Iteration 3 fit: [19.0 s]: Recall = 0.1952, Jaccard score = 0.1710, loss = 0.3477, eval: [7.8 s]
# Iteration 4 fit: [19.0 s]: Recall = 0.1958, Jaccard score = 0.1716, loss = 0.3315, eval: [7.9 s]
# Iteration 5 fit: [18.7 s]: Recall = 0.1953, Jaccard score = 0.1711, loss = 0.3150, eval: [7.8 s]
# Iteration 6 fit: [19.0 s]: Recall = 0.1905, Jaccard score = 0.1662, loss = 0.2967, eval: [7.9 s]
# Iteration 7 fit: [19.1 s]: Recall = 0.1892, Jaccard score = 0.1648, loss = 0.2783, eval: [7.9 s]
# Iteration 8 fit: [18.8 s]: Recall = 0.1828, Jaccard score = 0.1584, loss = 0.2576, eval: [7.9 s]
# Iteration 9 fit: [18.9 s]: Recall = 0.1749, Jaccard score = 0.1505, loss = 0.2377, eval: [7.9 s]
# Iteration 10 fit: [19.1 s]: Recall = 0.1724, Jaccard score = 0.1480, loss = 0.2187, eval: [7.8 s]
# Iteration 11 fit: [20.8 s]: Recall = 0.1679, Jaccard score = 0.1436, loss = 0.1996, eval: [7.9 s]
# Iteration 12 fit: [19.0 s]: Recall = 0.1641, Jaccard score = 0.1399, loss = 0.1811, eval: [7.9 s]
# Iteration 13 fit: [18.8 s]: Recall = 0.1650, Jaccard score = 0.1407, loss = 0.1651, eval: [7.8 s]
# Iteration 14 fit: [19.0 s]: Recall = 0.1605, Jaccard score = 0.1364, loss = 0.1501, eval: [7.9 s]
# Iteration 15 fit: [18.7 s]: Recall = 0.1603, Jaccard score = 0.1362, loss = 0.1354, eval: [7.9 s]
# Iteration 16 fit: [19.1 s]: Recall = 0.1556, Jaccard score = 0.1317, loss = 0.1234, eval: [8.0 s]
# Iteration 17 fit: [19.1 s]: Recall = 0.1565, Jaccard score = 0.1326, loss = 0.1127, eval: [7.9 s]
# Iteration 18 fit: [19.0 s]: Recall = 0.1518, Jaccard score = 0.1281, loss = 0.1020, eval: [8.0 s]
# Iteration 19 fit: [19.1 s]: Recall = 0.1509, Jaccard score = 0.1272, loss = 0.0942, eval: [7.9 s]
# Iteration 20 fit: [19.1 s]: Recall = 0.1509, Jaccard score = 0.1272, loss = 0.0870, eval: [7.9 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566464639.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566464639.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1094)         0           flatten_2[0][0]                  
#                                                                  user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# user_feature_item_latent (Dense (None, 512)          560640      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_item_latent_bn (Ba (None, 512)          2048        user_feature_item_latent[0][0]   
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           user_feature_item_latent_bn[0][0]
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 606)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 256)          155392      concatenate_2[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 256)          1024        final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            257         leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,787,361
# Trainable params: 2,785,825
# Non-trainable params: 1,536
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0288, Jaccard score = 0.0220
# Iteration 0 fit: [20.6 s]: Recall = 0.1750, Jaccard score = 0.1506, loss = 0.4203, eval: [7.9 s]
# Iteration 1 fit: [19.8 s]: Recall = 0.1916, Jaccard score = 0.1673, loss = 0.3822, eval: [7.9 s]
# Iteration 2 fit: [19.8 s]: Recall = 0.1984, Jaccard score = 0.1743, loss = 0.3614, eval: [7.9 s]
# Iteration 3 fit: [20.3 s]: Recall = 0.2016, Jaccard score = 0.1775, loss = 0.3440, eval: [7.9 s]
# Iteration 4 fit: [19.8 s]: Recall = 0.1996, Jaccard score = 0.1754, loss = 0.3281, eval: [8.1 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566464806.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566464806.h5
# Load data done [1.7 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1094)         0           flatten_2[0][0]                  
#                                                                  user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# user_feature_item_latent (Dense (None, 512)          560640      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_item_latent_bn (Ba (None, 512)          2048        user_feature_item_latent[0][0]   
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           user_feature_item_latent_bn[0][0]
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 606)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 96)           58272       concatenate_2[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 96)           384         final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,689,441
# Trainable params: 2,688,225
# Non-trainable params: 1,216
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0246, Jaccard score = 0.0187
# Iteration 0 fit: [20.5 s]: Recall = 0.1799, Jaccard score = 0.1555, loss = 0.4212, eval: [7.8 s]
# Iteration 1 fit: [19.6 s]: Recall = 0.1958, Jaccard score = 0.1716, loss = 0.3822, eval: [7.8 s]
# Iteration 2 fit: [19.6 s]: Recall = 0.1987, Jaccard score = 0.1745, loss = 0.3618, eval: [7.7 s]
# Iteration 3 fit: [19.6 s]: Recall = 0.2018, Jaccard score = 0.1777, loss = 0.3455, eval: [7.8 s]
# Iteration 4 fit: [19.6 s]: Recall = 0.1982, Jaccard score = 0.1740, loss = 0.3304, eval: [7.8 s]
# Iteration 5 fit: [19.6 s]: Recall = 0.1987, Jaccard score = 0.1745, loss = 0.3137, eval: [7.8 s]
# Iteration 6 fit: [19.6 s]: Recall = 0.1952, Jaccard score = 0.1710, loss = 0.2964, eval: [7.8 s]


# Launched by terminal.
# GMF arguments: Namespace(batch_size=256, big_tag=0, dataset='', dataset_name_prepend='cold_0.0_', early_stopping=45, epochs=300, eval_recall=1, is_tag=1, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='GMF', num_factors=94, num_k_folds=1, num_neg=4, out=1, path='../data/', percentage=0.0, reg_layers='[0,0,0,0]', reg_mf=0, test_dataset=1, topk=3, verbose=1) 
# The best NeuMF model will be saved to Pretrain/_GMF_94_[64,32,16,8]_1566465065.h5
--weights_path: Pretrain/_GMF_94_[64,32,16,8]_1566465065.h5
# Load data done [1.5 s]. #user=20000, #item=2000, #train=143502, #test=eval_recall
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to                     
# ==================================================================================================
# item_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# item_embedding (Embedding)      (None, 1, 94)        188000      item_input[0][0]                 
# __________________________________________________________________________________________________
# flatten_2 (Flatten)             (None, 94)           0           item_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_input (InputLayer) (None, 1000)         0                                            
# __________________________________________________________________________________________________
# user_input (InputLayer)         (None, 1)            0                                            
# __________________________________________________________________________________________________
# concatenate_1 (Concatenate)     (None, 1094)         0           flatten_2[0][0]                  
#                                                                  user_feature_input[0][0]         
# __________________________________________________________________________________________________
# user_embedding (Embedding)      (None, 1, 94)        1880000     user_input[0][0]                 
# __________________________________________________________________________________________________
# user_feature_item_latent (Dense (None, 512)          560640      concatenate_1[0][0]              
# __________________________________________________________________________________________________
# flatten_1 (Flatten)             (None, 94)           0           user_embedding[0][0]             
# __________________________________________________________________________________________________
# user_feature_item_latent_bn (Ba (None, 512)          2048        user_feature_item_latent[0][0]   
# __________________________________________________________________________________________________
# multiply_1 (Multiply)           (None, 94)           0           flatten_1[0][0]                  
#                                                                  flatten_2[0][0]                  
# __________________________________________________________________________________________________
# leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           user_feature_item_latent_bn[0][0]
# __________________________________________________________________________________________________
# concatenate_2 (Concatenate)     (None, 606)          0           multiply_1[0][0]                 
#                                                                  leaky_re_lu_1[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer (Dense)       (None, 96)           58272       concatenate_2[0][0]              
# __________________________________________________________________________________________________
# final_dense_layer_bn (BatchNorm (None, 96)           384         final_dense_layer[0][0]          
# __________________________________________________________________________________________________
# leaky_re_lu_2 (LeakyReLU)       (None, 96)           0           final_dense_layer_bn[0][0]       
# __________________________________________________________________________________________________
# prediction (Dense)              (None, 1)            97          leaky_re_lu_2[0][0]              
# ==================================================================================================
# Total params: 2,689,441
# Trainable params: 2,688,225
# Non-trainable params: 1,216
# __________________________________________________________________________________________________
# None
# 
# Performing k-fold 1
# Init: Recall = 0.0267, Jaccard score = 0.0204
# Iteration 0 fit: [20.5 s]: Recall = 0.1817, Jaccard score = 0.1573, loss = 0.4192, eval: [7.7 s]
# Iteration 1 fit: [19.7 s]: Recall = 0.1931, Jaccard score = 0.1688, loss = 0.3806, eval: [7.8 s]
# Iteration 2 fit: [19.6 s]: Recall = 0.1969, Jaccard score = 0.1727, loss = 0.3612, eval: [7.7 s]
# Iteration 3 fit: [19.7 s]: Recall = 0.2013, Jaccard score = 0.1772, loss = 0.3448, eval: [7.8 s]
# Iteration 4 fit: [19.7 s]: Recall = 0.2000, Jaccard score = 0.1759, loss = 0.3290, eval: [7.7 s]
