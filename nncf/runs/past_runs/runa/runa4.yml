--nn_model: MLP
--is_tag: "1"
--eval_recall: "1"
--topk: "3"
#--dataset: "ml-1m"
--big_tag: "0"
--epochs: "20"
--layers: "[32,16,8,4]"
#--mf_pretrain: "Pretrain/ml-1m_GMF_8_1501651698.h5"
#--mlp_pretrain: "Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5"


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', epochs=20, eval_recall=1, is_tag=1, layers='[32,16,8,4]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_neg=4, out=1, path='Data/', reg_layers='[0,0,0,0]', reg_mf=0, topk=3, verbose=1) 
# Load data done [2.7 s]. #user=17435, #item=986, #train=133152, #test=17435
# Init: Recall = 0.0040, Jaccard score = 0.0022
# Iteration 0 fit: [6.3 s]: Recall = 0.0055, Jaccard score = 0.0030, loss = 0.5461, val_loss = 0.0000, eval: [7.6 s]
# Iteration 1 fit: [5.8 s]: Recall = 0.0304, Jaccard score = 0.0167, loss = 0.4732, val_loss = 0.0000, eval: [7.9 s]
# Iteration 2 fit: [5.8 s]: Recall = 0.0294, Jaccard score = 0.0161, loss = 0.4527, val_loss = 0.0000, eval: [7.6 s]
# Iteration 3 fit: [5.8 s]: Recall = 0.0311, Jaccard score = 0.0171, loss = 0.4469, val_loss = 0.0000, eval: [7.8 s]
# Iteration 4 fit: [5.8 s]: Recall = 0.0256, Jaccard score = 0.0140, loss = 0.4417, val_loss = 0.0000, eval: [7.6 s]
# Iteration 5 fit: [5.8 s]: Recall = 0.0277, Jaccard score = 0.0152, loss = 0.4391, val_loss = 0.0000, eval: [7.7 s]
# Iteration 6 fit: [5.8 s]: Recall = 0.0209, Jaccard score = 0.0114, loss = 0.4378, val_loss = 0.0000, eval: [7.7 s]
# Iteration 7 fit: [5.8 s]: Recall = 0.0264, Jaccard score = 0.0144, loss = 0.4354, val_loss = 0.0000, eval: [7.6 s]
# Iteration 8 fit: [5.8 s]: Recall = 0.0281, Jaccard score = 0.0154, loss = 0.4329, val_loss = 0.0000, eval: [7.5 s]
# Iteration 9 fit: [5.8 s]: Recall = 0.0232, Jaccard score = 0.0126, loss = 0.4301, val_loss = 0.0000, eval: [8.2 s]
# Iteration 10 fit: [5.8 s]: Recall = 0.0239, Jaccard score = 0.0131, loss = 0.4268, val_loss = 0.0000, eval: [8.2 s]
# Iteration 11 fit: [5.8 s]: Recall = 0.0195, Jaccard score = 0.0107, loss = 0.4234, val_loss = 0.0000, eval: [7.6 s]
# Iteration 12 fit: [5.8 s]: Recall = 0.0184, Jaccard score = 0.0100, loss = 0.4192, val_loss = 0.0000, eval: [7.5 s]
# Iteration 13 fit: [5.8 s]: Recall = 0.0173, Jaccard score = 0.0094, loss = 0.4151, val_loss = 0.0000, eval: [7.6 s]
# Iteration 14 fit: [5.8 s]: Recall = 0.0161, Jaccard score = 0.0088, loss = 0.4109, val_loss = 0.0000, eval: [8.2 s]
# Iteration 15 fit: [5.8 s]: Recall = 0.0175, Jaccard score = 0.0095, loss = 0.4074, val_loss = 0.0000, eval: [7.6 s]
# Iteration 16 fit: [5.8 s]: Recall = 0.0178, Jaccard score = 0.0097, loss = 0.4031, val_loss = 0.0000, eval: [7.9 s]
# Iteration 17 fit: [5.8 s]: Recall = 0.0188, Jaccard score = 0.0102, loss = 0.3988, val_loss = 0.0000, eval: [7.5 s]
# Iteration 18 fit: [5.8 s]: Recall = 0.0184, Jaccard score = 0.0100, loss = 0.3946, val_loss = 0.0000, eval: [7.5 s]
# Iteration 19 fit: [5.8 s]: Recall = 0.0177, Jaccard score = 0.0096, loss = 0.3908, val_loss = 0.0000, eval: [8.2 s]
# End. Best Iteration 3:  Recall = 0.0311, Jaccard score = 0.0171. 
# The best NeuMF model is saved to Pretrain/_MLP_8_[32,16,8,4]_1559912904.h5


# Launched by terminal.
# MLP arguments: Namespace(batch_size=256, big_tag=0, dataset='', epochs=20, eval_recall=1, is_tag=1, layers='[32,16,8,4]', learner='adam', lr=0.001, mf_pretrain='', mlp_pretrain='', nn_model='MLP', num_factors=8, num_neg=4, out=1, path='Data/', reg_layers='[0,0,0,0]', reg_mf=0, topk=3, verbose=1) 
# Load data done [2.9 s]. #user=17425, #item=986, #train=133079, #test=17425
